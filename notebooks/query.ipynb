{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install neo4j\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "URI = os.getenv(\"NEO4J_URI\")\n",
    "USERNAME = os.getenv(\"NEO4J_USERNAME\")\n",
    "PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "# Function to run a query\n",
    "def run_query(tx, query, parameters={}):\n",
    "    return tx.run(query, parameters).data()\n",
    "\n",
    "def grap_read_query(query, parameters={}):\n",
    "    driver = GraphDatabase.driver(URI, auth=(USERNAME, PASSWORD))\n",
    "    with driver.session() as session:\n",
    "        result = session.execute_read(run_query, parameters)\n",
    "\n",
    "    driver.close()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pymilvus import connections, MilvusClient,model\n",
    "def emb_text(text):\n",
    "    model = SentenceTransformer(\"Muennighoff/SGPT-125M-weightedmean-nli-bitfit\")\n",
    "    embedding = model.encode([text], normalize_embeddings=True)\n",
    "    return embedding[0].tolist()\n",
    "\n",
    "def vector_query(query, limit, output_fields, collection_name): \n",
    "    # con = connections.connect(\n",
    "    # alias=\"default\",\n",
    "    # host='',\n",
    "    # port='19530'\n",
    "    # )    \n",
    "    milvus_client = MilvusClient(\"vector.db\") # Solo para servidor\n",
    "    search_res = milvus_client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[\n",
    "            emb_text(query)\n",
    "        ],  \n",
    "        limit=limit,\n",
    "        search_params={\"metric_type\": \"COSINE\", \"params\": {}},  # Inner product distance\n",
    "        output_fields=output_fields,  # Return the text field\n",
    "    )\n",
    "    return search_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realización de proceso de query completa\n",
    "def get_papers_and_citations(initial_query):\n",
    "    collection_name = \"papers\"\n",
    "    vector_result = vector_query(initial_query, 10, [\"Abstract\", \"Title\", \"TLDR\", \"Conference\"], collection_name)\n",
    "    paper_titles = [result[\"entity\"][\"Title\"] for result in vector_result]\n",
    "\n",
    "    query_graph = \"\"\"\n",
    "        MATCH r=(p:Paper)-[:CITES]-(:Paper)\n",
    "        WHERE p.title IN $paper_titles\n",
    "        RETURN r\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(URI, auth=(USERNAME, PASSWORD), database=\"merged\")\n",
    "\n",
    "    with driver.session() as session:\n",
    "        graph_result = session.execute_read(run_query, query_graph, {\"paper_titles\": paper_titles})\n",
    "\n",
    "    driver.close()\n",
    "    return {\n",
    "        \"vector\": vector_result,\n",
    "        \"graph\": graph_result\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_query(\"Serverless computing greatly simplifies the use of cloud resources. In particular, Function-as-a-Service (FaaS) platforms enable programmers to develop applications as individual functions that can run and scale independently. Unfortunately, applications that require fine-grained support for mutable state and synchronization, such as machine learning (ML) and scientific computing, are notoriously hard to build with this new paradigm. In this work, we aim at bridging this gap\", 100, [\"Abstract\", \"Title\", \"TLDR\", \"Conference\"], \"papers\")\n",
    "\n",
    "for result in results:\n",
    "    print(result[\"entity\"][\"Title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_results = get_papers_and_citations(\"Stateful serverless computing\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_matches(title, graph):\n",
    "    cites = []\n",
    "    cited_by = []\n",
    "    matches=[]\n",
    "    for match in graph:\n",
    "        if match[\"r\"][0][\"title\"] == title:\n",
    "            cites.append(match[\"r\"][-1])\n",
    "            matches.append(match[\"r\"])\n",
    "        elif match[\"r\"][-1][\"title\"] == title:\n",
    "            cited_by.append(match[\"r\"][0])\n",
    "            matches.append(match[\"r\"])\n",
    "    \n",
    "    return {\n",
    "        \"matches\": matches,\n",
    "        \"cites\": cites,\n",
    "        \"cited_by\": cited_by,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_merged_results(global_results):\n",
    "    merged_results = []\n",
    "    for vector in global_results[\"vector\"]:\n",
    "        citations = get_paper_matches(\n",
    "            vector[\"entity\"][\"Title\"], \n",
    "            global_results[\"graph\"]\n",
    "        )\n",
    "        matches = citations[\"matches\"]\n",
    "        cites = citations[\"cites\"]\n",
    "        cited_by = citations[\"cited_by\"]\n",
    "        if matches == []: \n",
    "            # continue\n",
    "            matches = None\n",
    "            cites = None\n",
    "            cited_by = None\n",
    "            community_id = None\n",
    "            connected_component_id = None\n",
    "            authors = None\n",
    "            predominant_country = None\n",
    "            predominant_continent = None\n",
    "        elif matches[0][0][\"title\"] == vector[\"entity\"][\"Title\"]:\n",
    "            community_id = matches[0][0][\"communityId\"] \n",
    "            connected_component_id = matches[0][0][\"connectedComponentId\"] \n",
    "            # authors = [{\"name\": author} for author in json.loads(matches[0][0][\"Authors\"])] \n",
    "            authors = [author for author in json.loads(matches[0][0][\"Authors\"])] \n",
    "            predominant_country = json.loads(matches[0][0][\"PredominantCountry\"])[0]\n",
    "            predominant_continent = json.loads(matches[0][0][\"PredominantContinent\"])[0]\n",
    "        else:\n",
    "            community_id = matches[0][-1][\"communityId\"]\n",
    "            connected_component_id = matches[0][-1][\"connectedComponentId\"]\n",
    "            # authors = [{\"name\": author} for author in json.loads( matches[0][-1][\"Authors\"] ) ]\n",
    "            authors = [ author for author in json.loads( matches[0][-1][\"Authors\"] ) ]\n",
    "            predominant_country = json.loads(matches[0][-1][\"PredominantCountry\"])[0]\n",
    "            predominant_continent = json.loads(matches[0][-1][\"PredominantContinent\"])[0]\n",
    "\n",
    "        merged_result = {\n",
    "            \"title\": vector[\"entity\"][\"Title\"],\n",
    "            \"abstract\": vector[\"entity\"][\"Abstract\"],\n",
    "            \"tldr\": vector[\"entity\"][\"TLDR\"],\n",
    "            \"conference\": vector[\"entity\"][\"Conference\"],\n",
    "            \"authors\": authors,\n",
    "            \"predominant_country\": predominant_country,\n",
    "            \"predominant_continent\": predominant_continent,\n",
    "            \"community_id\": community_id,\n",
    "            \"connected_component_id\": connected_component_id,\n",
    "            # \"matches\": matches,\n",
    "            \"cites\": cites,\n",
    "            \"cites_total\": len(cites) if cites is not None else 0,\n",
    "            # \"cited_by\": cited_by,\n",
    "            \"cited_by_total\": len(cited_by) if cited_by is not None else 0,\n",
    "        }\n",
    "        merged_results.append(merged_result)\n",
    "\n",
    "    return merged_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_results = get_papers_and_citations(\"Stateful serverless computing\")\n",
    "merged_results = get_merged_results(global_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicia sesión de spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, desc, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Milvus-PySpark\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial = spark.createDataFrame(merged_results)\n",
    "df_initial.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_communities_count_df(df):\n",
    "    return df.select(\"community_id\")\\\n",
    "        .where(col(\"community_id\").isNotNull())\\\n",
    "        .groupBy(\"community_id\")\\\n",
    "        .count()\\\n",
    "        .orderBy(\"count\", ascending=False)\n",
    "\n",
    "def get_connected_components_count_df(df):\n",
    "    return df.select(\"connected_component_id\")\\\n",
    "        .where(col(\"connected_component_id\").isNotNull())\\\n",
    "        .groupBy(\"connected_component_id\")\\\n",
    "        .count()\\\n",
    "        .orderBy(\"count\", ascending=False)\n",
    "\n",
    "def get_top_authors_df(df, top=3):\n",
    "    return df.select(\"authors\")\\\n",
    "        .where( col(\"author\").isNotNull())\\\n",
    "        .groupBy(\"author\")\\\n",
    "        .count()\\\n",
    "        .orderBy(\"count\", ascending=False)\\\n",
    "        .limit(top)\n",
    "\n",
    "def get_top_countries_df(df, top=3):\n",
    "    return df.select(\"predominant_country\")\\\n",
    "        .where( col(\"predominant_country\").isNotNull())\\\n",
    "        .groupBy(\"predominant_country\")\\\n",
    "        .count()\\\n",
    "        .orderBy(\"count\", ascending=False)\\\n",
    "        .limit(top)\n",
    "\n",
    "def get_top_institutions_df(df, top=3):\n",
    "    return df.select(\"institution\")\\\n",
    "        .where( col(\"institution\").isNotNull())\\\n",
    "        .groupBy(\"institution\")\\\n",
    "        .count()\\\n",
    "        .orderBy(\"count\", ascending=False)\\\n",
    "        .limit(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutinas posteriores al procesamiento inicial\n",
    "# Obtención de los nodos asociados a cada una de las componentes conexas\n",
    "def get_component_node_data(connected_components):\n",
    "    query_graph = \"\"\"\n",
    "        MATCH (p:Paper)-[:HAS_INSTITUTION]->(i:Institution)-[:LOCATED_IN]->(c:Country)\n",
    "        WHERE p.connectedComponentId IN $connected_components\n",
    "        RETURN p, i, c\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(URI, auth=(USERNAME, PASSWORD), database=\"merged\")\n",
    "\n",
    "    with driver.session() as session:\n",
    "        graph_result = session.execute_read(run_query, query_graph, {\"connected_components\": connected_components})\n",
    "\n",
    "    driver.close()\n",
    "    return graph_result\n",
    "\n",
    "# Obtención de los datos de comunidades\n",
    "def get_communities_node_data(communities):\n",
    "    query_graph = \"\"\"\n",
    "        MATCH (p:Paper)-[:HAS_INSTITUTION]->(i:Institution)-[:LOCATED_IN]->(c:Country)\n",
    "        WHERE p.communityId IN $communities\n",
    "        RETURN p, i, c\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(URI, auth=(USERNAME, PASSWORD), database=\"merged\")\n",
    "\n",
    "    with driver.session() as session:\n",
    "        graph_result = session.execute_read(run_query, query_graph, {\"communities\": communities})\n",
    "\n",
    "    driver.close()\n",
    "    return graph_result\n",
    "\n",
    "# Obtención conjunta de los datos relacionados con los nodos recuperados, por agregación\n",
    "def get_related_nodes_data(df):\n",
    "    communities = [row[\"community_id\"] for row in get_communities_count_df(df).select(\"community_id\").collect()]\n",
    "    connected_components = [row[\"connected_component_id\"] for row in get_connected_components_count_df(df).select(\"connected_component_id\").collect()]\n",
    "\n",
    "    communities_nodes = get_communities_node_data(communities)\n",
    "    connected_components_nodes = get_component_node_data(connected_components)\n",
    "\n",
    "    return {\n",
    "        \"communities\": communities_nodes,\n",
    "        \"connected_components\": connected_components_nodes,\n",
    "    }\n",
    "\n",
    "# Generación de DF de los datos relacionados con los nodos recuperados, por agregación\n",
    "def get_related_nodes_df(df):\n",
    "    nodes_data = get_related_nodes_data(df_initial)\n",
    "\n",
    "    communities_df = spark.createDataFrame([{ **node[\"p\"], \"institution\": node[\"i\"], \"country\": node[\"c\"] } for node in nodes_data[\"communities\"]])\n",
    "    connected_components_df = spark.createDataFrame([{ **node[\"p\"], \"institution\": node[\"i\"], \"country\": node[\"c\"] }  for node in nodes_data[\"connected_components\"]])\n",
    "\n",
    "    return {\n",
    "        \"communities\": communities_df,\n",
    "        \"connected_components\": connected_components_df\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculos relacionados con las comunidades y componentes conexas\n",
    "df_initial.persist()\n",
    "related_nodes_dfs = get_related_nodes_df(df_initial)\n",
    "communities_df = related_nodes_dfs[\"communities\"]\n",
    "connected_components_df = related_nodes_dfs[\"connected_components\"]\n",
    "\n",
    "df_initial.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_df.unpersist()\n",
    "communities_df.persist()\n",
    "\n",
    "# Obtenciónn de las pripcipales instituciones\n",
    "window = Window.partitionBy(\"community_id\", \"institution_name\").orderBy(\"count\")\n",
    "\n",
    "communities_top_institution_df = communities_df\\\n",
    "    .select(\"title\", col(\"communityId\").alias(\"community_id\"), col(\"institution.name\").alias(\"institution_name\"))\\\n",
    "    .distinct()\\\n",
    "    .groupBy(\"community_id\", \"institution_name\")\\\n",
    "    .count()\\\n",
    "    .withColumn(\"row_number\", row_number().over(window))\\\n",
    "    .orderBy(\"count\", ascending=False)\\\n",
    "    .select(\"community_id\", \"institution_name\", \"count\")\\\n",
    "    .where(col(\"row_number\") <= 5)\n",
    "\n",
    "window = Window.partitionBy(\"community_id\").orderBy(desc(\"count\"))\n",
    "communities_top_countries_df = communities_df\\\n",
    "    .select(\"title\", col(\"communityId\").alias(\"community_id\"), col(\"country.name\").alias(\"country_name\"))\\\n",
    "    .distinct()\\\n",
    "    .groupBy(\"community_id\", \"country_name\")\\\n",
    "    .count()\\\n",
    "    .withColumn(\"row_number\", row_number().over(window))\\\n",
    "    .orderBy(\"community_id\", desc(\"count\"))\\\n",
    "    .select(\"community_id\", \"country_name\", \"count\")\\\n",
    "    .where(col(\"row_number\") <= 5)\n",
    "\n",
    "communities_df.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_top_institution_df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_df.select(\"institution.name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
