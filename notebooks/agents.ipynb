{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc422e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.papers.io import db\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "# TODO variables de entorno y no usar caminos relativos\n",
    "EXTENDED_CRAWLER_DATA_PATH = os.getenv(\"EXTENDED_CRAWLER_DATA_PATH\")\n",
    "MILVUS_DB = os.getenv(\"MILVUS_DB\")\n",
    "MILVUS_COLLECTION = os.getenv(\"MILVUS_COLLECTION\")\n",
    "MILVUS_ALIAS = os.getenv(\"MILVUS_ALIAS\")\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\")\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\")\n",
    "\n",
    "milvus_client = db.Milvus(\n",
    "    # db=MILVUS_DB, \n",
    "    collection=MILVUS_COLLECTION, \n",
    "    alias=MILVUS_ALIAS,\n",
    "    host=MILVUS_HOST,\n",
    "    port=MILVUS_PORT, \n",
    "    new_collection=False\n",
    ")   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecb10c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Spot instances\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd723116",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_papers = milvus_client.search(\n",
    "    text=query,\n",
    "    output_fields=[\n",
    "        \"Title\",\n",
    "        \"TLDR\",\n",
    "        \"Abstract\",\n",
    "        \"KeyConcepts\",\n",
    "        \"Year\",\n",
    "        \"Conference\",\n",
    "        \"Summary\",\n",
    "        \"AuthorsAndInstitutions\"\n",
    "    ],\n",
    "    limit=100,\n",
    "    hybrid=True,\n",
    "    hybrid_fields=[\n",
    "        \"AbstractVector\", \n",
    "        \"TitleVector\", \n",
    "        \"TLDRVector\",\n",
    "        \"KeyConceptsVector\"\n",
    "    ],\n",
    "    expr=\"Year in ['2023']\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [paper[\"entity\"] for paper in nn_papers]\n",
    "for paper in papers:\n",
    "    print(paper[\"Year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e990bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87020ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = model.invoke([HumanMessage(content=\"hi!\")])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd34a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = []\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "response = model_with_tools.invoke([HumanMessage(content=\"Hi!\")])\n",
    "\n",
    "print(f\"ContentString: {response.content}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")\n",
    "\n",
    "response = model_with_tools.invoke([HumanMessage(content=\"What's the weather in SF?\")])\n",
    "\n",
    "print(f\"ContentString: {response.content}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba9a4a5",
   "metadata": {},
   "source": [
    "### Agente bÃ¡sico\n",
    "Las funciones a utilizar son proporcionada por medio de tools. Estas toman un input a partir de la query y son devueltos\n",
    "\n",
    "Se debe tipifizar la salida con pydantic, que es proporcionada al agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "911aec5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhoIsResponse(person='Jaimito', lives_in='Madrid', age=7)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Base agent\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from pydantic import BaseModel\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"qwen3\")\n",
    "\n",
    "def get_person(person: str) -> str:  \n",
    "    \"\"\"Get person information.\"\"\"\n",
    "    return f\"Jaimito is my cousin, lives in Madrid, he is 7 years old\"\n",
    "\n",
    "class WhoIsResponse(BaseModel):\n",
    "    person: str\n",
    "    lives_in: str\n",
    "    age: int\n",
    "    \n",
    "tools = [get_person]\n",
    "agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    response_format=WhoIsResponse,\n",
    "    prompt=\"You are a helpful assistant that uses only the feedback you are provided\",  \n",
    ")\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Who is Jaimito?\"}]}\n",
    ")\n",
    "\n",
    "response[\"structured_response\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1390eea",
   "metadata": {},
   "source": [
    "### Minimal workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b9cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1:8b\")\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    print(\"weather\")\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "def who_is(text: str) -> str:  \n",
    "    \"\"\"who is person information.\"\"\"\n",
    "    print(\"person\")\n",
    "    return \"Jaimito is my cousin, lives in Madrid, he is 7 years old\"\n",
    "\n",
    "# class DescriptionResponse(BaseModel):\n",
    "#     person: str\n",
    "#     lives_in: str\n",
    "#     age: int\n",
    "\n",
    "# agent = create_react_agent(\n",
    "#     model=model,\n",
    "#     tools=[get_weather],\n",
    "#     prompt=\"You are a helpful assistant\"\n",
    "# )\n",
    "\n",
    "tools = []\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# Tell the LLM which tools it can call\n",
    "llm_with_tools = model.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    print(state)\n",
    "    return {\"messages\": [model.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "# Add tool node\n",
    "# tool_node = ToolNode(tools)\n",
    "# graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Add condition to call a node or another\n",
    "# graph_builder.add_conditional_edges(\n",
    "#     \"chatbot\",\n",
    "#     tools_condition,\n",
    "# )\n",
    "\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever\n",
    "# the node is used.\n",
    "# graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d9d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114266d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13daa938",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph_updates(\"Spot instances papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108cadc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d7791ba",
   "metadata": {},
   "source": [
    "### Prototipo de agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f8b2ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<think>\\n\\n</think>\\n\\n{\\n    \"main_concept\": \"serverless cloud computing\",\\n    \"years\": [\\n        {\\n            \"value\": \"2022\",\\n            \"equal\": True,\\n            \"citation\": False\\n        },\\n        {\\n            \"value\": \"2023\",\\n            \"equal\": False,\\n            \"citation\": True\\n        }\\n    ],\\n    \"countries\": [\\n        {\\n            \"value\": \"DE\",\\n            \"equal\": True,\\n            \"citation\": False\\n        }\\n    ],\\n    \"conferences\": [\\n        {\\n            \"value\": \"NSDI\",\\n            \"equal\": True,\\n            \"citation\": False\\n        }\\n    ]\\n}' additional_kwargs={} response_metadata={'model': 'qwen3', 'created_at': '2025-05-28T20:01:22.558509177Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2501749320, 'load_duration': 12906874, 'prompt_eval_count': 431, 'prompt_eval_duration': 37570963, 'eval_count': 140, 'eval_duration': 2447817977, 'model_name': 'qwen3'} id='run-9df388ee-d64e-4855-9c14-295ea86e0070-0' usage_metadata={'input_tokens': 431, 'output_tokens': 140, 'total_tokens': 571}\n",
      "main_concept='serverless cloud computing' authors=None institutions=None countries=None years=[FilterOptions(value='2022', equal=True, citation=False), FilterOptions(value='2023', equal=False, citation=True)] conferences=[FilterOptions(value='NSDI', equal=True, citation=False)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Base agent\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "query = \"Which papers were published in 2022 in nsdi related to serverless cloud computing? Select only those from Germany. Omit also citations from 2023\"\n",
    "model = ChatOllama(model=\"qwen3\", temperature=0, top_k=20, top_p=0.8)\n",
    "\n",
    "class FilterOptions(BaseModel):\n",
    "    \"\"\"\n",
    "    Class for filter option. \n",
    "    - value corresponds for to the value to be used in filtering\n",
    "    - equal indicates if the condition indicates equality\n",
    "    - citation indicates if this condition is referred only to cited papers\n",
    "    \"\"\"\n",
    "    value: str\n",
    "    equal: bool\n",
    "    citation: bool\n",
    "\n",
    "class FilterParameters(BaseModel):\n",
    "    main_concept: str = None\n",
    "    authors: list[FilterOptions] = None\n",
    "    institutions: list[FilterOptions] = None\n",
    "    countries: list[FilterOptions] = None\n",
    "    years: list[FilterOptions] = None\n",
    "    conferences: list[FilterOptions] = None\n",
    "    \n",
    "def get_query_filter_conditions(query: str, model: ChatOllama) -> FilterParameters:\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(\"\"\"\n",
    "        You are a helpful assistant that uses only the input the user provides. Your role is to help research paper data related to the topics the user asks about.\n",
    "        Fields with list of FilterOptions type must be filled as follows:\n",
    "        - value: The value of the field to be filled\n",
    "        - equal: False if it is asked to be different from the value\n",
    "        - citation: If the field value refers to paper citations\n",
    "        Return JSON with (if not mentioned, leave it as None):\n",
    "        - main_concept (string). Topic of research\n",
    "        - years (list[FilterOptions]). Publish years\n",
    "        - authors (list[FilterOptions]). List of paper authors\n",
    "        - institutions (list[FilterOptions]). All must be valid institutions laike universities or corporations.\n",
    "        - countries (list[FilterOptions]). All must be valid countries. Translate them to usual abbreviaions like US for United Staes of America or DE for Germany\n",
    "        - conferences (list[FilterOptions]). List of valid conferences: IEEECloud, Middleware, SIGCOMM, eurosys\n",
    "        Example response for 2024 papers about spot instances with 100+ citations published in middleware. Authors must be from Harvard University. Cited papers must not be from Canada:\n",
    "        {{\n",
    "            \"main_concept\": \"spot instances\",\n",
    "            \"authors\": None,\n",
    "            \"institutions\": [{{ \"value\": \"Harvard University\", \"equal\": True, \"citation\": False}}],\n",
    "            \"countries\": [{{\"value\": \"CA\", \"equal\": False, \"citation\": True}}],\n",
    "            \"years\": [{{\"value\": \"2025\", \"equal\": True, \"citation\": False}}],\n",
    "            \"conferences\": [{{\"value\": \"Middleware\", \"equal\": True, \"citation\": False}}],\n",
    "        }}\n",
    "        /no_think\n",
    "        \"\"\"),\n",
    "        HumanMessage(query),\n",
    "    ]\n",
    "    # response = model.with_structured_output(FilterParameters).invoke(messages)\n",
    "    response = model.invoke(messages)\n",
    "    print(response)\n",
    "    # return response\n",
    "    messages_for_structure = [\n",
    "        messages[0],\n",
    "        HumanMessage(f\"\"\" \n",
    "        Return JSON with (if not mentioned, leave it as None):\n",
    "        - value: The value of the field to be filled\n",
    "        - equal: False if it is asked to be different from the value\n",
    "        - citation: If the field value refers to paper citations\n",
    "        Return JSON with (if not mentioned, leave it as None):\n",
    "        - main_concept (string). Topic of research\n",
    "        - years (list[FilterOptions]). Publish years\n",
    "        - authors (list[FilterOptions]). List of paper authors\n",
    "        - institutions (list[FilterOptions]). All must be valid institutions laike universities or corporations.\n",
    "        - countries (list[FilterOptions]). All must be valid countries. Translate them to usual abbreviaions like US for United Staes of America or DE for Germany\n",
    "        - conferences (list[FilterOptions]). List of valid conferences: IEEECloud, Middleware, SIGCOMM, eurosys\n",
    "        Using these previous result:\n",
    "        {response}\n",
    "        /no_think\n",
    "        \"\"\"\n",
    "        )\n",
    "    ]\n",
    "    structured_response = model.with_structured_output(FilterParameters).invoke(messages_for_structure)\n",
    "    print(structured_response)\n",
    "    \n",
    "    return structured_response\n",
    "\n",
    "filter_conditions = get_query_filter_conditions(query, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "678d4f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year in ['2022']\n"
     ]
    }
   ],
   "source": [
    "def build_expr_for_filter_field(filter_options: list[FilterOptions], field_name: str, json_field: bool = False) -> str:\n",
    "    expr = \"\"\n",
    "    in_expr = []\n",
    "    not_in_expr = []\n",
    "\n",
    "    if not filter_options:\n",
    "        return \"\"\n",
    "    \n",
    "    for filter_option in filter_options:\n",
    "        if not filter_option.citation:\n",
    "            if filter_option.equal:\n",
    "                in_expr.append(filter_option.value)\n",
    "            else:\n",
    "                not_in_expr.append(filter_option.value)\n",
    "            \n",
    "    if len(in_expr) > 0:\n",
    "        if len(expr) == 0:\n",
    "            if json_field:\n",
    "                expr = f\"json_contains_any({field_name}, {in_expr})\"\n",
    "            else:\n",
    "                expr = f\"{field_name} in {in_expr}\"\n",
    "        else:\n",
    "            if len(expr) == 0:\n",
    "                if json_field:            \n",
    "                    expr += f\"and json_contains_any({field_name}, {in_expr})\"\n",
    "                else:\n",
    "                    expr += f\"and {field_name} in {in_expr}\"\n",
    "\n",
    "    if len(not_in_expr) > 0:\n",
    "        if len(expr) == 0:\n",
    "            if json_field:\n",
    "                expr = f\"not json_contains_any({field_name}, {not_in_expr})\"\n",
    "            else:\n",
    "                expr = f\"not {field_name} in {not_in_expr}\"\n",
    "        else:\n",
    "            if len(expr) == 0:\n",
    "                if json_field:            \n",
    "                    expr += f\"and not json_contains_any({field_name}, {not_in_expr})\"\n",
    "                else:\n",
    "                    expr += f\"and not {field_name} in {not_in_expr}\"\n",
    "    return expr\n",
    "\n",
    "expr_years = build_expr_for_filter_field(filter_options=filter_conditions.years, field_name=\"Year\")\n",
    "expr_authors = build_expr_for_filter_field(filter_options=filter_conditions.authors, field_name=\"Authors\", json_field=True)\n",
    "expr_countries = build_expr_for_filter_field(filter_options=filter_conditions.countries, field_name=\"Countries\", json_field=True)\n",
    "expr_institution = build_expr_for_filter_field(filter_options=filter_conditions.institutions, field_name=\"Institutions\", json_field=True)\n",
    "expr_conferences = build_expr_for_filter_field(filter_options=filter_conditions.conferences, field_name=\"Conferences\")\n",
    "\n",
    "expr = \"\"\n",
    "if len(expr_years) > 0:\n",
    "    if len(expr) == 0:\n",
    "        expr = expr_years\n",
    "    else:\n",
    "        expr += \" and \" + expr_years\n",
    "        \n",
    "if len(expr_authors) > 0:\n",
    "    if len(expr) == 0:\n",
    "        expr = expr_authors\n",
    "    else:\n",
    "        expr += \" and \" + expr_authors\n",
    "        \n",
    "if len(expr_countries) > 0:\n",
    "    if len(expr) == 0:\n",
    "        expr = expr_countries\n",
    "    else:\n",
    "        expr += \" and \" + expr_countries\n",
    "        \n",
    "if len(expr_institution) > 0:\n",
    "    if len(expr) == 0:\n",
    "        expr = expr_institution\n",
    "    else:\n",
    "        expr += \" and \" + expr_institution\n",
    "        \n",
    "# if len(expr_conferences) > 0:\n",
    "#     if len(expr) == 0:\n",
    "#         expr = expr_conferences\n",
    "#     else:\n",
    "#         expr += \" and \" + expr_conferences                                \n",
    "\n",
    "print(expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "429cff32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[{'Author': 'Wes Lloyd', 'Institutions': [{'Institution Name': 'University of Washington Tacoma', 'Country': 'US'}]}, {'Author': 'Shruti Ramesh', 'Institutions': [{'Institution Name': 'Microsoft (United States)', 'Country': 'US'}]}, {'Author': 'Swetha Chinthalapati', 'Institutions': [{'Institution Name': 'University of Washington Tacoma', 'Country': 'US'}]}, {'Author': 'Lan H. Ly', 'Institutions': [{'Institution Name': 'University of Washington Tacoma', 'Country': 'US'}]}, {'Author': 'Shrideep Pallickara', 'Institutions': [{'Institution Name': 'Colorado State University', 'Country': 'US'}]}]\n"
     ]
    }
   ],
   "source": [
    "nn_papers = milvus_client.search(\n",
    "    text=filter_conditions.main_concept,\n",
    "    output_fields=[\n",
    "        \"Title\",\n",
    "        \"TLDR\",\n",
    "        \"Abstract\",\n",
    "        \"KeyConcepts\",\n",
    "        \"Year\",\n",
    "        \"Conference\",\n",
    "        \"Summary\",\n",
    "        \"AuthorsAndInstitutions\"\n",
    "    ],\n",
    "    limit=100,\n",
    "    hybrid=True,\n",
    "    hybrid_fields=[\n",
    "        \"AbstractVector\", \n",
    "        \"TitleVector\", \n",
    "        \"TLDRVector\",\n",
    "        \"KeyConceptsVector\"\n",
    "    ],\n",
    "    # expr=expr\n",
    "    expr=\"\"\n",
    ")\n",
    "if len(nn_papers) > 0:\n",
    "    print(len(nn_papers))\n",
    "    print(nn_papers[0][\"entity\"][\"AuthorsAndInstitutions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7283bf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_valid=True\n",
      "is_valid=True\n",
      "is_valid=True\n",
      "is_valid=True\n",
      "is_valid=True\n",
      "is_valid=True\n",
      "is_valid=True\n",
      "is_valid=False\n",
      "is_valid=True\n",
      "is_valid=True\n",
      "is_valid=True\n",
      "is_valid=False\n",
      "is_valid=False\n",
      "is_valid=True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     38\u001b[39m             valid_papers.append(entity)\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m valid_papers\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m valid_papers = \u001b[43mfilter_papers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilter_conditions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmain_concept\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpapers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnn_papers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(valid_papers))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mfilter_papers\u001b[39m\u001b[34m(topic, papers, model)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m paper \u001b[38;5;129;01min\u001b[39;00m papers:\n\u001b[32m     33\u001b[39m     entity = paper[\u001b[33m\"\u001b[39m\u001b[33mentity\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     check_result = \u001b[43mcheck_paper_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(check_result)\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check_result.is_valid:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mcheck_paper_topic\u001b[39m\u001b[34m(topic, paper, model)\u001b[39m\n\u001b[32m     10\u001b[39m tldr = paper[\u001b[33m\"\u001b[39m\u001b[33mTLDR\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m paper[\u001b[33m\"\u001b[39m\u001b[33mTLDR\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m messages = [\n\u001b[32m     12\u001b[39m     SystemMessage(\u001b[33m\"\"\"\u001b[39m\u001b[33mYou are a strict assistant. Your role is to help determine if papers discuss the topic the user asks about. \u001b[39m\n\u001b[32m     13\u001b[39m \u001b[33m    Only accept those that strictly mention the topic since a mistake will kill all your family. If in doubt is it better to determine a paper as not valid.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m),\n\u001b[32m     26\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m response = \u001b[43mmodel_with_structure\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3032\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3030\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3031\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3032\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3033\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3034\u001b[39m         \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:5416\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5409\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5410\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5411\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5414\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5415\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5417\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5418\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5419\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5420\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:368\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    356\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    358\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    363\u001b[39m     **kwargs: Any,\n\u001b[32m    364\u001b[39m ) -> BaseMessage:\n\u001b[32m    365\u001b[39m     config = ensure_config(config)\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    367\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    378\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:937\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    930\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    934\u001b[39m     **kwargs: Any,\n\u001b[32m    935\u001b[39m ) -> LLMResult:\n\u001b[32m    936\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m937\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:759\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    757\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    758\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    765\u001b[39m         )\n\u001b[32m    766\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    767\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1002\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1000\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1006\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/langchain_ollama/chat_models.py:715\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    709\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    710\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    713\u001b[39m     **kwargs: Any,\n\u001b[32m    714\u001b[39m ) -> ChatResult:\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    718\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m    719\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m    720\u001b[39m         message=AIMessage(\n\u001b[32m    721\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m    726\u001b[39m         generation_info=generation_info,\n\u001b[32m    727\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/langchain_ollama/chat_models.py:652\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    644\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    645\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    649\u001b[39m     **kwargs: Any,\n\u001b[32m    650\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    651\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m652\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterate_over_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/langchain_ollama/chat_models.py:737\u001b[39m, in \u001b[36mChatOllama._iterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iterate_over_stream\u001b[39m(\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    732\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m    733\u001b[39m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    734\u001b[39m     **kwargs: Any,\n\u001b[32m    735\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m    736\u001b[39m     is_thinking = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/langchain_ollama/chat_models.py:639\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    636\u001b[39m chat_params = \u001b[38;5;28mself\u001b[39m._chat_params(messages, stop, **kwargs)\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/ollama/_client.py:163\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m      \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/contextlib.py:141\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/httpx/_client.py:868\u001b[39m, in \u001b[36mClient.stream\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    845\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    846\u001b[39m \u001b[33;03mAlternative to `httpx.request()` that streams the response body\u001b[39;00m\n\u001b[32m    847\u001b[39m \u001b[33;03minstead of loading it into memory at once.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    853\u001b[39m \u001b[33;03m[0]: /quickstart#streaming-responses\u001b[39;00m\n\u001b[32m    854\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    855\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    856\u001b[39m     method=method,\n\u001b[32m    857\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    866\u001b[39m     extensions=extensions,\n\u001b[32m    867\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    875\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tfm/.venv/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "class TopicCheck(BaseModel):\n",
    "    is_valid: bool\n",
    "    # reason: str\n",
    "    \n",
    "\n",
    "def check_paper_topic(topic: str, paper: dict, model: ChatOllama) -> dict:\n",
    "    model_with_structure = model.with_structured_output(TopicCheck)\n",
    "    title = paper[\"Title\"]\n",
    "    abstract = paper[\"Abstract\"] if paper[\"Abstract\"] else \"\"\n",
    "    tldr = paper[\"TLDR\"] if paper[\"TLDR\"] else \"\"\n",
    "    messages = [\n",
    "        SystemMessage(\"\"\"You are a strict assistant. Your role is to help determine if papers discuss the topic the user asks about. \n",
    "        Only accept those that strictly mention the topic since a mistake will kill all your family. If in doubt is it better to determine a paper as not valid.\n",
    "        Return JSON with (if not mentioned, leave it as None):\n",
    "        - is_valid (bool). True if the paper data is about the topic requested\n",
    "        Example response for paper with abstract 'Kappa proposes a framework for simplified serverless development using checkpointing to handle timeouts and providing concurrency mechanisms for parallel' and topic 'serverless development':\n",
    "        {{\n",
    "            \"is_valid\": true,\n",
    "        }}  \n",
    "        \"\"\"),\n",
    "        HumanMessage(f\"\"\"Determine if the following paper content is about the topic: <topic>{topic}</topic>\\n\n",
    "        Title: {title}\\n\n",
    "        TLDR: {tldr}\\n\n",
    "        Abstract: {abstract}\\n\n",
    "        \"\"\"),\n",
    "    ]\n",
    "    response = model_with_structure.invoke(messages)\n",
    "    return response\n",
    "\n",
    "def filter_papers(topic: str, papers: list[dict], model: ChatOllama) -> list[str]:\n",
    "    valid_papers = []\n",
    "    for paper in papers:\n",
    "        entity = paper[\"entity\"]\n",
    "        \n",
    "        check_result = check_paper_topic(topic, entity, model)\n",
    "        print(check_result)\n",
    "        if check_result.is_valid:\n",
    "            valid_papers.append(entity)\n",
    "            \n",
    "    return valid_papers\n",
    "            \n",
    "valid_papers = filter_papers(topic=filter_conditions.main_concept, papers=nn_papers, model=model)\n",
    "print(len(valid_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89e1ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"qwen3:0.6b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2eba024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chain_0': TopicCheck(is_valid=True), 'chain_1': TopicCheck(is_valid=True), 'chain_2': TopicCheck(is_valid=False), 'chain_3': TopicCheck(is_valid=False), 'chain_4': TopicCheck(is_valid=False), 'chain_5': TopicCheck(is_valid=True), 'chain_6': TopicCheck(is_valid=False), 'chain_7': TopicCheck(is_valid=True), 'chain_8': TopicCheck(is_valid=True), 'chain_9': TopicCheck(is_valid=False), 'chain_10': TopicCheck(is_valid=False), 'chain_11': TopicCheck(is_valid=True), 'chain_12': TopicCheck(is_valid=True), 'chain_13': TopicCheck(is_valid=True), 'chain_14': TopicCheck(is_valid=False), 'chain_15': TopicCheck(is_valid=True), 'chain_16': TopicCheck(is_valid=True), 'chain_17': TopicCheck(is_valid=True), 'chain_18': TopicCheck(is_valid=False), 'chain_19': TopicCheck(is_valid=True), 'chain_20': TopicCheck(is_valid=True), 'chain_21': TopicCheck(is_valid=True), 'chain_22': TopicCheck(is_valid=True), 'chain_23': TopicCheck(is_valid=True), 'chain_24': TopicCheck(is_valid=True), 'chain_25': TopicCheck(is_valid=False), 'chain_26': TopicCheck(is_valid=True), 'chain_27': TopicCheck(is_valid=True), 'chain_28': TopicCheck(is_valid=True), 'chain_29': TopicCheck(is_valid=True), 'chain_30': TopicCheck(is_valid=True), 'chain_31': TopicCheck(is_valid=True), 'chain_32': TopicCheck(is_valid=True), 'chain_33': TopicCheck(is_valid=False), 'chain_34': TopicCheck(is_valid=False), 'chain_35': TopicCheck(is_valid=False), 'chain_36': TopicCheck(is_valid=True), 'chain_37': TopicCheck(is_valid=True), 'chain_38': TopicCheck(is_valid=False), 'chain_39': TopicCheck(is_valid=False), 'chain_40': TopicCheck(is_valid=False), 'chain_41': TopicCheck(is_valid=False), 'chain_42': TopicCheck(is_valid=True), 'chain_43': TopicCheck(is_valid=False), 'chain_44': TopicCheck(is_valid=True), 'chain_45': TopicCheck(is_valid=False), 'chain_46': TopicCheck(is_valid=True), 'chain_47': TopicCheck(is_valid=False), 'chain_48': TopicCheck(is_valid=False), 'chain_49': TopicCheck(is_valid=True), 'chain_50': TopicCheck(is_valid=True), 'chain_51': TopicCheck(is_valid=True), 'chain_52': TopicCheck(is_valid=True), 'chain_53': TopicCheck(is_valid=True), 'chain_54': TopicCheck(is_valid=True), 'chain_55': TopicCheck(is_valid=True), 'chain_56': TopicCheck(is_valid=False), 'chain_57': TopicCheck(is_valid=False), 'chain_58': TopicCheck(is_valid=True), 'chain_59': TopicCheck(is_valid=True), 'chain_60': TopicCheck(is_valid=True), 'chain_61': TopicCheck(is_valid=False), 'chain_62': TopicCheck(is_valid=False), 'chain_63': TopicCheck(is_valid=True), 'chain_64': TopicCheck(is_valid=True), 'chain_65': TopicCheck(is_valid=True), 'chain_66': TopicCheck(is_valid=True), 'chain_67': TopicCheck(is_valid=True), 'chain_68': TopicCheck(is_valid=True), 'chain_69': TopicCheck(is_valid=False), 'chain_70': TopicCheck(is_valid=True), 'chain_71': TopicCheck(is_valid=False), 'chain_72': TopicCheck(is_valid=True), 'chain_73': TopicCheck(is_valid=True), 'chain_74': TopicCheck(is_valid=True), 'chain_75': TopicCheck(is_valid=True), 'chain_76': TopicCheck(is_valid=False), 'chain_77': TopicCheck(is_valid=True), 'chain_78': TopicCheck(is_valid=False), 'chain_79': TopicCheck(is_valid=False), 'chain_80': TopicCheck(is_valid=False), 'chain_81': TopicCheck(is_valid=False), 'chain_82': TopicCheck(is_valid=False), 'chain_83': TopicCheck(is_valid=False), 'chain_84': TopicCheck(is_valid=True), 'chain_85': TopicCheck(is_valid=False), 'chain_86': TopicCheck(is_valid=True), 'chain_87': TopicCheck(is_valid=True), 'chain_88': TopicCheck(is_valid=True), 'chain_89': TopicCheck(is_valid=True), 'chain_90': TopicCheck(is_valid=False), 'chain_91': TopicCheck(is_valid=False), 'chain_92': TopicCheck(is_valid=False), 'chain_93': TopicCheck(is_valid=False), 'chain_94': TopicCheck(is_valid=False), 'chain_95': TopicCheck(is_valid=True), 'chain_96': TopicCheck(is_valid=True), 'chain_97': TopicCheck(is_valid=False), 'chain_98': TopicCheck(is_valid=False), 'chain_99': TopicCheck(is_valid=True)}\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "# Paralallell\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "valid_papers = []\n",
    "index = 0\n",
    "map_chain_elements = {}\n",
    "for paper in nn_papers:\n",
    "    entity = paper[\"entity\"]\n",
    "    \n",
    "    topic = filter_conditions.main_concept\n",
    "    \n",
    "    model_with_structure = model.with_structured_output(TopicCheck)\n",
    "    title = entity[\"Title\"]\n",
    "    abstract = entity[\"Abstract\"] if entity[\"Abstract\"] else \"\"\n",
    "    tldr = entity[\"TLDR\"] if entity[\"TLDR\"] else \"\"\n",
    "    messages = [\n",
    "        SystemMessage(\"\"\"You are a strict assistant. Your role is to help determine if papers discuss the topic the user asks about. \n",
    "        Only accept those that strictly mention the topic since a mistake will kill all your family. If in doubt is it better to determine a paper as not valid.\n",
    "        Return JSON with (if not mentioned, leave it as None):\n",
    "        - is_valid (bool). True if the paper data is about the topic requested\n",
    "        Example response for paper with abstract 'Kappa proposes a framework for simplified serverless development using checkpointing to handle timeouts and providing concurrency mechanisms for parallel' and topic 'serverless development':\n",
    "        {{\n",
    "            \"is_valid\": true,\n",
    "        }}  \n",
    "        \"\"\"),\n",
    "        HumanMessage(f\"\"\"Determine if the following paper content is about the topic: <topic>{topic}</topic>\\n\n",
    "        Title: {title}\\n\n",
    "        TLDR: {tldr}\\n\n",
    "        Abstract: {abstract}\\n\n",
    "        \"\"\")\n",
    "    ]\n",
    "    key = f\"chain_{index}\"\n",
    "    index += 1\n",
    "    chain = (\n",
    "        ChatPromptTemplate.from_messages(messages)\n",
    "        | model_with_structure\n",
    "    )\n",
    "    # print(chain)\n",
    "    map_chain_elements[key] = chain\n",
    "    # print(map_chain_elements)\n",
    "\n",
    "# print(map_chain_elements)\n",
    "map_chain = RunnableParallel(map_chain_elements)\n",
    "response = map_chain.invoke({})\n",
    "print(response)\n",
    "valid_papers = []\n",
    "for index, paper in enumerate(nn_papers):\n",
    "    chain_i = f\"chain_{index}\"\n",
    "    if response[chain_i].is_valid:\n",
    "        valid_papers.append(paper)\n",
    "        \n",
    "print(len(valid_papers))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7082aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = nn_papers[0][\"entity\"]\n",
    "\n",
    "print(entity[\"Title\"])\n",
    "print(entity[\"Abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"qwen3\", temperature=0)\n",
    "\n",
    "class AggregationInQueryChecker(BaseModel):\n",
    "    requests_aggregation: bool\n",
    "    aggregations_requested: list[str]\n",
    "    \n",
    "\n",
    "def check_aggregations_requested(query: str, model: ChatOllama) -> dict:\n",
    "    model_with_structure = model.with_structured_output(AggregationInQueryChecker)\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(\"\"\"You are a helpful AI assistant that uses only the feedback you are provided. Your role is to help in custom searches of scientific papers. Determine if in the provided query data aggregations are requested.\n",
    "        An example would be \"Provide the most cited papers of every country relating spot instances\", which is requesting an aggregation by country\n",
    "        Return JSON with the following properties:\n",
    "        - requests_aggregation (bool). True if the query requests data aggregations\n",
    "        - aggregations_requested (list[string]). List containing the expected aggregations. The possible values are the following ones: citations, papers, country, conference, institution, author\n",
    "        Example response for the query 'Provide the most cited papers of every country relating spot instances in conference Middleware':\n",
    "        {{\n",
    "            \"requests_aggregation\": true,\n",
    "            \"aggregations_requested\": [\"citations\", \"country\"]\n",
    "        }}  \n",
    "        \"\"\"),\n",
    "        HumanMessage(f\"\"\"Determine the aggregations requested in the following query: <query>{query}</query>:\\n\"\"\"),\n",
    "    ]\n",
    "    response = model_with_structure.invoke(messages)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc21045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "check_aggregations_requested(query=query, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ade9719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving citations: 100%|ââââââââââ| 100/100 [00:02<00:00, 41.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.papers.domain.citations_analyzer import CitationAnalyzer\n",
    "import polars as pl\n",
    "\n",
    "### Dynamic query\n",
    "neo4j_client = db.Neo4j(\"bolt://localhost:7687\", \"neo4j\", \"password\", \"middleware\")\n",
    "citations_analyzer = CitationAnalyzer(graph_client=neo4j_client)\n",
    "\n",
    "papers = [paper[\"entity\"] for paper in nn_papers]\n",
    "df_citations = citations_analyzer.process_papers(papers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1eb8a1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.head of shape: (29, 8)\n",
       "ââââââââââââââ¬âââââââââââââ¬âââââââââââââ¬ââââââââââââ¬ââââââââââââ¬ââââââââââââ¬ââââââââââââ¬ââââââââââââ\n",
       "â source_tit â source_yea â source_con â source_pr â cited_tit â cited_pre â cited_con â cited_cou â\n",
       "â le         â r          â ference    â edominant â le        â dominant_ â ference   â ntry      â\n",
       "â ---        â ---        â ---        â _country  â ---       â country   â ---       â ---       â\n",
       "â str        â str        â str        â ---       â str       â ---       â str       â str       â\n",
       "â            â            â            â list[str] â           â list[str] â           â           â\n",
       "ââââââââââââââªâââââââââââââªâââââââââââââªââââââââââââªââââââââââââªââââââââââââªââââââââââââªââââââââââââ¡\n",
       "â Orchestrat â 2012       â nsdi       â [\"DE\"]    â Proceedin â []        â UNKNOWN   â UNKNOWN   â\n",
       "â ing the    â            â            â           â gs of the â           â           â           â\n",
       "â Deployment â            â            â           â 5th       â           â           â           â\n",
       "â oâ¦         â            â            â           â Symposiâ¦  â           â           â           â\n",
       "â Orchestrat â 2012       â nsdi       â [\"DE\"]    â Grid      â []        â Australas â UNKNOWN   â\n",
       "â ing the    â            â            â           â Resource  â           â ian       â           â\n",
       "â Deployment â            â            â           â Allocatio â           â Symposium â           â\n",
       "â oâ¦         â            â            â           â n : Allâ¦  â           â on Gridâ¦  â           â\n",
       "â Orchestrat â 2012       â nsdi       â [\"DE\"]    â Predictin â []        â UNKNOWN   â UNKNOWN   â\n",
       "â ing the    â            â            â           â g and Opt â           â           â           â\n",
       "â Deployment â            â            â           â imizing   â           â           â           â\n",
       "â oâ¦         â            â            â           â Systâ¦     â           â           â           â\n",
       "â Orchestrat â 2012       â nsdi       â [\"DE\"]    â Effective â [\"US\"]    â Measureme â US        â\n",
       "â ing the    â            â            â           â distribut â           â nt and    â           â\n",
       "â Deployment â            â            â           â ed        â           â Modeling  â           â\n",
       "â oâ¦         â            â            â           â scheduliâ¦ â           â of Coâ¦    â           â\n",
       "â Orchestrat â 2012       â nsdi       â [\"DE\"]    â Don't     â []        â USENIX    â UNKNOWN   â\n",
       "â ing the    â            â            â           â Settle    â           â Workshop  â           â\n",
       "â Deployment â            â            â           â for Less  â           â on Hot    â           â\n",
       "â oâ¦         â            â            â           â Than theâ¦ â           â Topics â¦  â           â\n",
       "â â¦          â â¦          â â¦          â â¦         â â¦         â â¦         â â¦         â â¦         â\n",
       "â Orchestrat â 2012       â nsdi       â [\"DE\"]    â Making    â [\"US\"]    â ACM       â US        â\n",
       "â ing the    â            â            â           â cloud int â           â Symposium â           â\n",
       "â Deployment â            â            â           â ermediate â           â on Cloud  â           â\n",
       "â oâ¦         â            â            â           â dataâ¦     â           â Computiâ¦  â           â\n",
       "â Orchestrat â 2012       â nsdi       â [\"DE\"]    â Cloudward â [\"US\"]    â Conferenc â US        â\n",
       "â ing the    â            â            â           â bound:    â           â e on Appl â           â\n",
       "â Deployment â            â            â           â planning  â           â ications, â           â\n",
       "â oâ¦         â            â            â           â for â¦     â           â Teâ¦       â           â\n",
       "â Orchestrat â 2012       â nsdi       â [\"DE\"]    â Cloudward â [\"US\"]    â Conferenc â US        â\n",
       "â ing the    â            â            â           â bound:    â           â e on Appl â           â\n",
       "â Deployment â            â            â           â planning  â           â ications, â           â\n",
       "â oâ¦         â            â            â           â for â¦     â           â Teâ¦       â           â\n",
       "â Orchestrat â 2012       â nsdi       â [\"DE\"]    â Cloudward â [\"US\"]    â Conferenc â US        â\n",
       "â ing the    â            â            â           â bound:    â           â e on Appl â           â\n",
       "â Deployment â            â            â           â planning  â           â ications, â           â\n",
       "â oâ¦         â            â            â           â for â¦     â           â Teâ¦       â           â\n",
       "â Orchestrat â 2012       â nsdi       â [\"DE\"]    â Mesos: A  â []        â Symposium â UNKNOWN   â\n",
       "â ing the    â            â            â           â Platform  â           â on        â           â\n",
       "â Deployment â            â            â           â for       â           â Networked â           â\n",
       "â oâ¦         â            â            â           â Fine-Graâ¦ â           â Systemsâ¦  â           â\n",
       "ââââââââââââââ´âââââââââââââ´âââââââââââââ´ââââââââââââ´ââââââââââââ´ââââââââââââ´ââââââââââââ´ââââââââââââ>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_citations.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07d7ee5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<think>\\n\\n</think>\\n\\n```json\\n{\\n  \"main_concept\": \"serverless cloud computing\",\\n  \"years\": [{{\"value\": \"2022\", \"equal\": True, \"citation\": False}}],\\n  \"institutions\": [{{\"value\": \"Germany\", \"equal\": True, \"citation\": False}}],\\n  \"countries\": [{{\"value\": \"DE\", \"equal\": True, \"citation\": False}}],\\n  \"conferences\": []\\n}\\n```' additional_kwargs={} response_metadata={'model': 'qwen3:0.6b', 'created_at': '2025-05-27T18:40:25.004611874Z', 'done': True, 'done_reason': 'stop', 'total_duration': 522663385, 'load_duration': 15363218, 'prompt_eval_count': 431, 'prompt_eval_duration': 18224986, 'eval_count': 99, 'eval_duration': 485636578, 'model_name': 'qwen3:0.6b'} id='run-896ff90c-1c03-4ee9-92a6-19b2c44900fb-0' usage_metadata={'input_tokens': 431, 'output_tokens': 99, 'total_tokens': 530}\n",
      "main_concept='serverless cloud computing' authors=None institutions=None countries=None years=[FilterOptions(value='2022', equal=True, citation=False)] conferences=[FilterOptions(value='Middleware', equal=False, citation=False)]\n"
     ]
    }
   ],
   "source": [
    "filter_conditions = get_query_filter_conditions(query, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e2de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def dynamic_filter(filters: list[dict], df: pl.DataFrame) -> pl.DataFrame:\n",
    "    df_filtered = df\n",
    "    for filter in filters:\n",
    "        print(filter)\n",
    "        if filter[\"equal\"]:\n",
    "            df_filtered = df_filtered.filter(\n",
    "                pl.col(filter[\"field\"]).is_in(filter[\"values\"])\n",
    "            )\n",
    "        else:\n",
    "            df_filtered = df_filtered.filter(\n",
    "                ~pl.col(filter[\"field\"]).is_in(filter[\"values\"])\n",
    "            )\n",
    "    return df_filtered\n",
    "\n",
    "def dynamic_aggregation(columns: list[str], df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return (\n",
    "        df.group_by(columns)\n",
    "        .agg(pl.count().alias(\"total_citations\"))\n",
    "    )\n",
    "    \n",
    "    # authors: list[FilterOptions] = None\n",
    "    # institutions: list[FilterOptions] = None\n",
    "    # countries: list[FilterOptions] = None\n",
    "    # years: list[FilterOptions] = None\n",
    "    # conferences: list[FilterOptions] = None\n",
    "def process_paper_data(filter_parameters: FilterParameters, df:pl.DataFrame) -> pl.DataFrame:\n",
    "    df_filters = []\n",
    "    if filter_parameters.authors and len(filter_parameters.authors) > 0:\n",
    "        df_filter_authors = map_filter_options(filter_options=filter_parameters.authors, source_field=\"source_author\", cited_field=\"cited_author\")\n",
    "        for filter in df_filter_authors:\n",
    "            df_filters.append(filter)\n",
    "            \n",
    "    if filter_parameters.institutions and len(filter_parameters.institutions) > 0:\n",
    "        df_filter_institutions = map_filter_options(filter_options=filter_parameters.institutions, source_field=\"source_institution\", cited_field=\"cited_institution\")\n",
    "        for filter in df_filter_institutions:\n",
    "            df_filters.append(filter)\n",
    "            model = ChatOllama(model=\"qwen3\", temperature=0)\n",
    "\n",
    "class AggregationInQueryChecker(BaseModel):\n",
    "    requests_aggregation: bool\n",
    "    aggregations_requested: list[str]\n",
    "    \n",
    "\n",
    "def check_aggregations_requested(query: str, model: ChatOllama) -> dict:\n",
    "    model_with_structure = model.with_structured_output(AggregationInQueryChecker)\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(\"\"\"You are a helpful AI assistant that uses only the feedback you are provided. Your role is to help in custom searches of scientific papers. Determine if in the provided query data aggregations are requested.\n",
    "        An example would be \"Provide the most cited papers of every country relating spot instances\", which is requesting an aggregation by country\n",
    "        Return JSON with the following properties:\n",
    "        - requests_aggregation (bool). True if the query requests data aggregations\n",
    "        - aggregations_requested (list[string]). List containing the expected aggregations. The possible values are the following ones: citations, papers, country, conference, institution, author\n",
    "        Example response for the query 'Provide the most cited papers of every country relating spot instances in conference Middleware':\n",
    "        {{\n",
    "            \"requests_aggregation\": true,\n",
    "            \"aggregations_requested\": [\"citations\", \"country\"]\n",
    "        }}  \n",
    "        \"\"\"),\n",
    "        HumanMessage(f\"\"\"Determine the aggregations requested in the following query: <query>{query}</query>:\\n\"\"\"),\n",
    "    ]\n",
    "    response = model_with_structure.invoke(messages)\n",
    "    return response\n",
    "    if filter_parameters.countries and len(filter_parameters.countries) > 0:\n",
    "        df_filter_countries = map_filter_options(filter_options=filter_parameters.countries, source_field=\"source_country\", cited_field=\"cited_country\")\n",
    "        for filter in df_filter_countries:\n",
    "            df_filters.append(filter)\n",
    "            \n",
    "    if filter_parameters.years and len(filter_parameters.years) > 0:\n",
    "        df_filter_years = map_filter_options(filter_options=filter_parameters.years, source_field=\"source_year\", cited_field=\"cited_year\")\n",
    "        for filter in df_filter_years:\n",
    "            df_filters.append(filter)                                    \n",
    "            \n",
    "    if filter_parameters.conferences and len(filter_parameters.conferences) > 0:\n",
    "        df_filter_conferences = map_filter_options(filter_options=filter_parameters.conferences, source_field=\"source_conference\", cited_field=\"cited_conference\")\n",
    "        for filter in df_filter_conferences:\n",
    "            df_filters.append(filter)      \n",
    "            \n",
    "    df_filtered = dynamic_filter(filters=df_filters, df=df)\n",
    "    df_aggregated = dynamic_aggregation(columns=[\"cited_country\"], df=df_filtered)\n",
    "    return df_aggregated           \n",
    "\n",
    "def map_filter_options(filter_options: list[FilterOptions], source_field, cited_field: str) -> list[dict]:\n",
    "    mapping = {\n",
    "        \"cited_equal\": [],\n",
    "        \"cited_not_equal\": [],\n",
    "        \"source_equal\": [],\n",
    "        \"source_not_equal\": [],\n",
    "    }\n",
    "    df_filters = []\n",
    "    for filter_option in filter_options:\n",
    "        if filter_option.citation:\n",
    "            mapping[\"cited_equal\"].append(filter_option.value) if filter_option.equal else mapping[\"cited_not_equal\"].append(filter_option.value)\n",
    "        else:            \n",
    "            mapping[\"source_equal\"].append(filter_option.value) if filter_option.equal else mapping[\"source_not_equal\"].append(filter_option.value)\n",
    "            \n",
    "    if len(mapping[\"source_equal\"]) > 0:\n",
    "        df_filters.append({\"field\": source_field, \"values\": mapping[\"source_equal\"], \"equal\": True})\n",
    "        \n",
    "    if len(mapping[\"source_not_equal\"]) > 0:\n",
    "        df_filters.append({\"field\": source_field, \"values\": mapping[\"source_not_equal\"], \"equal\": False})\n",
    "        \n",
    "    if len(mapping[\"cited_equal\"]) > 0:            \n",
    "        df_filters.append({\"field\": cited_field, \"values\": mapping[\"cited_equal\"], \"equal\": True})\n",
    "        \n",
    "    if len(mapping[\"cited_not_equal\"]) > 0:\n",
    "        df_filters.append({\"field\": cited_field, \"values\": mapping[\"cited_not_equal\"], \"equal\": False})    \n",
    "    return df_filters\n",
    "    \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b2ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_paper_data(filter_parameters=filter_conditions, df=df_citations).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa817eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregated = dynamic_aggregation(\n",
    "    columns=[\"source_year\", \"source_title\", \"source_conference\"],\n",
    "    df=df_filtered\n",
    ")\n",
    "df_aggregated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f6a8fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302680f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.agents import create_structured_chat_agent\n",
    "\n",
    "\n",
    "model = ChatOllama(model=\"qwen3\", temperature=0)\n",
    "\n",
    "agent = create_structured_chat_agent(\n",
    "    llm = model,\n",
    "    tools=[],\n",
    "    prompt\n",
    ")\n",
    "\n",
    "def check_aggregations_requested(query: str, model: ChatOllama) -> dict:\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad7273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool='CountryCitationAnalysis' tool_input={} log='<think>\\n\\n</think>\\n\\n```\\n{\\n  \"action\": \"CountryCitationAnalysis\",\\n  \"action_input\": {}\\n}\\n```'\n",
      "tool='PublicationTrendAnalysis' tool_input={'year_range': [2010, 2020]} log='<think>\\n\\n</think>\\n\\n```\\n{\\n  \"action\": \"PublicationTrendAnalysis\",\\n  \"action_input\": {\\n    \"year_range\": [2010, 2020]\\n  }\\n}\\n```'\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_structured_chat_agent\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field\n",
    "import polars as pl\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Define your tools with structured inputs\n",
    "class CitationAnalysisTool(BaseModel):\n",
    "    pass  # No input needed for this example\n",
    "\n",
    "class PublicationTrendTool(BaseModel):\n",
    "    year_range: list[int] = Field(\n",
    "        None, \n",
    "        description=\"Optional year range filter [start_year, end_year]\"\n",
    "    )\n",
    "\n",
    "def get_most_cited_countries(df: pl.DataFrame) -> dict:\n",
    "    \"\"\"Get top cited countries from the dataframe\"\"\"\n",
    "    return (\n",
    "        df.filter(pl.col(\"cited_country\") != \"UNKNOWN\")\n",
    "        .group_by(\"cited_country\")\n",
    "        .agg(pl.len().alias(\"citation_count\"))\n",
    "        .sort(\"citation_count\", descending=True)\n",
    "        .to_dicts()\n",
    "    )\n",
    "\n",
    "def get_publications_per_year(df: pl.DataFrame, year_range: list = None) -> dict:\n",
    "    \"\"\"Get publication count by year with optional filtering\"\"\"\n",
    "    base = df.filter(pl.col(\"source_year\").cast(int).is_not_null())\n",
    "    \n",
    "    if year_range:\n",
    "        base = base.filter(\n",
    "            pl.col(\"source_year\").cast(int).is_between(year_range[0], year_range[1])\n",
    "        )\n",
    "    \n",
    "    return (\n",
    "        base.group_by(\"source_year\")\n",
    "        .agg(pl.len().alias(\"publication_count\"))\n",
    "        .sort(\"source_year\")\n",
    "        .to_dicts()\n",
    "    )\n",
    "\n",
    "# Create structured tools\n",
    "tools = [\n",
    "    StructuredTool.from_function(\n",
    "        func=lambda _: get_most_cited_countries(df_citations),\n",
    "        name=\"CountryCitationAnalysis\",\n",
    "        description=\"Analyze citation patterns by country\",\n",
    "        args_schema=CitationAnalysisTool\n",
    "    ),\n",
    "    StructuredTool.from_function(\n",
    "        func=lambda yr: get_publications_per_year(df_citations, yr),\n",
    "        name=\"PublicationTrendAnalysis\",\n",
    "        description=\"Analyze publication trends over years\",\n",
    "        args_schema=PublicationTrendTool\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize the agent\n",
    "llm = ChatOllama(model=\"qwen3\", temperature=0, top_k=20, top_p=0.8)\n",
    "system = '''Respond to the human as helpfully and accurately as possible. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n",
    "\n",
    "Valid \"action\" values: \"Final Answer\" or {tool_names}\n",
    "\n",
    "Provide only ONE action per $JSON_BLOB, as shown:\n",
    "\n",
    "```\n",
    "{{model = ChatOllama(model=\"qwen3\", temperature=0)\n",
    "```\n",
    "\n",
    "Follow this format:\n",
    "\n",
    "Question: input question to answer\n",
    "Thought: consider previous and subsequent steps\n",
    "Action:\n",
    "```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: action result\n",
    "... (repeat Thought/Action/Observation N times)\n",
    "Thought: I know what to respond\n",
    "Action:\n",
    "```\n",
    "{{\n",
    "  \"action\": \"Final Answer\",\n",
    "  \"action_input\": \"Final response to human\"\n",
    "}}\n",
    "\n",
    "Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation /no_think'''\n",
    "\n",
    "human = '''{input}\n",
    "\n",
    "{agent_scratchpad}\n",
    "\n",
    "(reminder to respond in a JSON blob no matter what) /no_think'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        # MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", human),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent = create_structured_chat_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "def process_query(query: str):\n",
    "    response = agent.invoke({\n",
    "        \"input\": f\"Analyze this research query: {query}\",\n",
    "        \"intermediate_steps\": []\n",
    "    })\n",
    "    return response\n",
    "\n",
    "# Test queries\n",
    "print(process_query(\"Which countries are most cited in cloud research?\"))\n",
    "print(process_query(\"Show me publication trends between 2010 and 2020\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_cited_countries(text) -> str:\n",
    "    \"\"\"Get top cited countries from the dataframe\"\"\"\n",
    "    print(\"tool 1\")\n",
    "    df = (\n",
    "        df_citations.filter(pl.col(\"cited_country\") != \"UNKNOWN\")\n",
    "        .group_by([\"cited_country\"])\\\n",
    "        .agg(pl.count().alias(\"citation_count\"))\\\n",
    "        .sort(\"citation_count\", descending=True)\\\n",
    "        .limit(10)\n",
    "    )\n",
    "    with pl.Config(\n",
    "        tbl_formatting=\"MARKDOWN\",\n",
    "        tbl_hide_column_data_types=True,\n",
    "        tbl_hide_dataframe_shape=True,\n",
    "    ):    \n",
    "        return \"Here are the most cited countries in markdown format:  \\n\" + str((df).head(10))\n",
    "def get_publications_per_year(text) -> dict:\n",
    "    \"\"\"Get publication count by year with optional filtering\"\"\"\n",
    "    print(\"tool 2\")\n",
    "    return \"Don't have enough data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "800396e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model = ChatOllama(model=\"llama3.2\", temperature=0.6, top_k=20, top_p=0.8)\n",
    "tools = [get_most_cited_countries, get_publications_per_year]\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "data = df_citations.to_dicts()\n",
    "\n",
    "response = model_with_tools.invoke([HumanMessage(f\"\"\"\n",
    "Use the data enclosed in <context> to answer the question asked enclosed in <query>.\n",
    "If further information is needed use the following tools {tools}\n",
    "<context>\n",
    "{\"\"}\n",
    "</context>\n",
    "<query>\n",
    "Tell me the country most publications per year\n",
    "</query>\n",
    "/no_think\n",
    "\"\"\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5846d61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99105/3202056457.py:1: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  response.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'content': '',\n",
       " 'additional_kwargs': {},\n",
       " 'response_metadata': {'model': 'llama3.2',\n",
       "  'created_at': '2025-05-27T20:46:22.850225613Z',\n",
       "  'done': True,\n",
       "  'done_reason': 'stop',\n",
       "  'total_duration': 2413852857,\n",
       "  'load_duration': 2036660245,\n",
       "  'prompt_eval_count': 297,\n",
       "  'prompt_eval_duration': 185220669,\n",
       "  'eval_count': 23,\n",
       "  'eval_duration': 191486300,\n",
       "  'model_name': 'llama3.2'},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run-6c4ba4d1-b75c-486f-bf28-dff36d8b2353-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [{'name': 'get_most_cited_countries',\n",
       "   'args': {'text': 'The country with the most publications per year'},\n",
       "   'id': 'c99be3ef-acb3-4c7d-a2d1-b667991690ee',\n",
       "   'type': 'tool_call'}],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 297,\n",
       "  'output_tokens': 23,\n",
       "  'total_tokens': 320}}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3344c147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>source_title</th><th>source_year</th><th>source_conference</th><th>source_predominant_country</th><th>cited_title</th><th>cited_predominant_country</th><th>cited_conference</th><th>cited_country</th></tr><tr><td>str</td><td>str</td><td>str</td><td>list[str]</td><td>str</td><td>list[str]</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;Serverless Computing: An Invesâ¦</td><td>&quot;2018&quot;</td><td>&quot;ic2e&quot;</td><td>[&quot;US&quot;]</td><td>&quot;Empirical prediction models foâ¦</td><td>[&quot;AU&quot;]</td><td>&quot;Future generations computer syâ¦</td><td>&quot;HK&quot;</td></tr><tr><td>&quot;Serverless Computing: An Invesâ¦</td><td>&quot;2018&quot;</td><td>&quot;ic2e&quot;</td><td>[&quot;US&quot;]</td><td>&quot;Empirical prediction models foâ¦</td><td>[&quot;AU&quot;]</td><td>&quot;Future generations computer syâ¦</td><td>&quot;AU&quot;</td></tr><tr><td>&quot;Serverless Computing: An Invesâ¦</td><td>&quot;2018&quot;</td><td>&quot;ic2e&quot;</td><td>[&quot;US&quot;]</td><td>&quot;Empirical prediction models foâ¦</td><td>[&quot;AU&quot;]</td><td>&quot;Future generations computer syâ¦</td><td>&quot;AU&quot;</td></tr><tr><td>&quot;Serverless Computing: An Invesâ¦</td><td>&quot;2018&quot;</td><td>&quot;ic2e&quot;</td><td>[&quot;US&quot;]</td><td>&quot;Initial Findings for Provisionâ¦</td><td>[&quot;QA&quot;]</td><td>&quot;2010 IEEE Second Internationalâ¦</td><td>&quot;QA&quot;</td></tr><tr><td>&quot;Serverless Computing: An Invesâ¦</td><td>&quot;2018&quot;</td><td>&quot;ic2e&quot;</td><td>[&quot;US&quot;]</td><td>&quot;A Measurement Study of Server â¦</td><td>[&quot;US&quot;]</td><td>&quot;2011 IEEE Ninth International â¦</td><td>&quot;US&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 8)\n",
       "ââââââââââââââ¬âââââââââââââ¬âââââââââââââ¬ââââââââââââ¬ââââââââââââ¬ââââââââââââ¬ââââââââââââ¬ââââââââââââ\n",
       "â source_tit â source_yea â source_con â source_pr â cited_tit â cited_pre â cited_con â cited_cou â\n",
       "â le         â r          â ference    â edominant â le        â dominant_ â ference   â ntry      â\n",
       "â ---        â ---        â ---        â _country  â ---       â country   â ---       â ---       â\n",
       "â str        â str        â str        â ---       â str       â ---       â str       â str       â\n",
       "â            â            â            â list[str] â           â list[str] â           â           â\n",
       "ââââââââââââââªâââââââââââââªâââââââââââââªââââââââââââªââââââââââââªââââââââââââªââââââââââââªââââââââââââ¡\n",
       "â Serverless â 2018       â ic2e       â [\"US\"]    â Empirical â [\"AU\"]    â Future    â HK        â\n",
       "â Computing: â            â            â           â predictio â           â generatio â           â\n",
       "â An Invesâ¦  â            â            â           â n models  â           â ns        â           â\n",
       "â            â            â            â           â foâ¦       â           â computer  â           â\n",
       "â            â            â            â           â           â           â syâ¦       â           â\n",
       "â Serverless â 2018       â ic2e       â [\"US\"]    â Empirical â [\"AU\"]    â Future    â AU        â\n",
       "â Computing: â            â            â           â predictio â           â generatio â           â\n",
       "â An Invesâ¦  â            â            â           â n models  â           â ns        â           â\n",
       "â            â            â            â           â foâ¦       â           â computer  â           â\n",
       "â            â            â            â           â           â           â syâ¦       â           â\n",
       "â Serverless â 2018       â ic2e       â [\"US\"]    â Empirical â [\"AU\"]    â Future    â AU        â\n",
       "â Computing: â            â            â           â predictio â           â generatio â           â\n",
       "â An Invesâ¦  â            â            â           â n models  â           â ns        â           â\n",
       "â            â            â            â           â foâ¦       â           â computer  â           â\n",
       "â            â            â            â           â           â           â syâ¦       â           â\n",
       "â Serverless â 2018       â ic2e       â [\"US\"]    â Initial   â [\"QA\"]    â 2010 IEEE â QA        â\n",
       "â Computing: â            â            â           â Findings  â           â Second    â           â\n",
       "â An Invesâ¦  â            â            â           â for Provi â           â Internati â           â\n",
       "â            â            â            â           â sionâ¦     â           â onalâ¦     â           â\n",
       "â Serverless â 2018       â ic2e       â [\"US\"]    â A Measure â [\"US\"]    â 2011 IEEE â US        â\n",
       "â Computing: â            â            â           â ment      â           â Ninth Int â           â\n",
       "â An Invesâ¦  â            â            â           â Study of  â           â ernationa â           â\n",
       "â            â            â            â           â Server â¦  â           â l â¦       â           â\n",
       "ââââââââââââââ´âââââââââââââ´âââââââââââââ´ââââââââââââ´ââââââââââââ´ââââââââââââ´ââââââââââââ´ââââââââââââ"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_citations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "375eea55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99105/3695064024.py:7: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  .agg(pl.count().alias(\"citation_count\"))\\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'| cited_country | citation_count |\\n|---------------|----------------|\\n| US            | 1845           |\\n| DE            | 203            |\\n| GB            | 199            |\\n| CN            | 198            |\\n| FR            | 134            |\\n| CH            | 119            |\\n| ES            | 116            |\\n| CA            | 97             |\\n| NL            | 88             |\\n| IT            | 87             |'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_cited_countries(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d0987241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99105/986490537.py:3: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  .agg(pl.count().alias(\"citation_count\"))\\\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>cited_country</th><th>citation_count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;US&quot;</td><td>1845</td></tr><tr><td>&quot;DE&quot;</td><td>203</td></tr><tr><td>&quot;GB&quot;</td><td>199</td></tr><tr><td>&quot;CN&quot;</td><td>198</td></tr><tr><td>&quot;FR&quot;</td><td>134</td></tr><tr><td>&quot;CH&quot;</td><td>119</td></tr><tr><td>&quot;ES&quot;</td><td>116</td></tr><tr><td>&quot;CA&quot;</td><td>97</td></tr><tr><td>&quot;NL&quot;</td><td>88</td></tr><tr><td>&quot;IT&quot;</td><td>87</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 2)\n",
       "âââââââââââââââââ¬âââââââââââââââââ\n",
       "â cited_country â citation_count â\n",
       "â ---           â ---            â\n",
       "â str           â u32            â\n",
       "âââââââââââââââââªâââââââââââââââââ¡\n",
       "â US            â 1845           â\n",
       "â DE            â 203            â\n",
       "â GB            â 199            â\n",
       "â CN            â 198            â\n",
       "â FR            â 134            â\n",
       "â CH            â 119            â\n",
       "â ES            â 116            â\n",
       "â CA            â 97             â\n",
       "â NL            â 88             â\n",
       "â IT            â 87             â\n",
       "âââââââââââââââââ´âââââââââââââââââ"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_citations.filter(pl.col(\"cited_country\") != \"UNKNOWN\")\\\n",
    "    .group_by([\"cited_title\", \"cited_country\"])\\\n",
    "    .agg(pl.count().alias(\"citation_count\"))\\\n",
    "    .group_by([\"cited_country\"])\\\n",
    "    .agg(pl.sum(\"citation_count\").alias(\"citation_count\"))\\\n",
    "    .sort(\"citation_count\", descending=True)\\\n",
    "    .limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490ed01d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_citations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mdf_citations\u001b[49m.filter(pl.col(\u001b[33m\"\u001b[39m\u001b[33mcited_country\u001b[39m\u001b[33m\"\u001b[39m) != \u001b[33m\"\u001b[39m\u001b[33mUNKNOWN\u001b[39m\u001b[33m\"\u001b[39m)\\\n\u001b[32m      2\u001b[39m     .unique([\u001b[33m\"\u001b[39m\u001b[33msource_title\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mcited_title\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcited_country\u001b[39m\u001b[33m\"\u001b[39m])\\\n\u001b[32m      3\u001b[39m     .group_by([\u001b[33m\"\u001b[39m\u001b[33msource_title\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mcited_title\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcited_country\u001b[39m\u001b[33m\"\u001b[39m])\\\n\u001b[32m      4\u001b[39m     .agg(pl.count().alias(\u001b[33m\"\u001b[39m\u001b[33mcitation_count\u001b[39m\u001b[33m\"\u001b[39m))\\\n\u001b[32m      5\u001b[39m     .sort(\u001b[33m\"\u001b[39m\u001b[33msource_title\u001b[39m\u001b[33m\"\u001b[39m, descending=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pl.Config(\n\u001b[32m      8\u001b[39m     tbl_formatting=\u001b[33m\"\u001b[39m\u001b[33mMARKDOWN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     tbl_hide_column_data_types=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     10\u001b[39m     tbl_hide_dataframe_shape=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     11\u001b[39m ):\n\u001b[32m     12\u001b[39m     st = \u001b[38;5;28mstr\u001b[39m((df).head(\u001b[32m10\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'df_citations' is not defined"
     ]
    }
   ],
   "source": [
    "df = df_citations.filter(pl.col(\"cited_country\") != \"UNKNOWN\")\\\n",
    "    .unique([\"source_title\",\"cited_title\", \"cited_country\"])\\\n",
    "    .group_by([\"source_title\",\"cited_title\", \"cited_country\"])\\\n",
    "    .agg(pl.count().alias(\"citation_count\"))\\\n",
    "    .sort(\"source_title\", descending=True)\n",
    "\n",
    "with pl.Config(\n",
    "    tbl_formatting=\"MARKDOWN\",\n",
    "    tbl_hide_column_data_types=True,\n",
    "    tbl_hide_dataframe_shape=True,\n",
    "):\n",
    "    st = str((df).head(10))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f0c9356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_ollama import ChatOllama\n",
    "import polars as pl\n",
    "\n",
    "model = ChatOllama(model=\"qwen3\", temperature=0.2)\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "class AgentTools:\n",
    "    def __init__(self, df: pl.DataFrame):\n",
    "        self.df = df\n",
    "    def get_most_cited_countries(self, text: str):\n",
    "        \"\"\"Get top cited countries from the dataframe\"\"\"\n",
    "        print(\"Uso de tool 1\")\n",
    "        return \"Most cited country is USA\"\n",
    "    def get_publications_per_year(self, text: str):\n",
    "        \"\"\"Get publication count by year with optional filtering\"\"\"\n",
    "        print(\"Uso de tool 2\")\n",
    "        return \"Most publications done in 2022\"\n",
    "    \n",
    "    \n",
    "agent_tools = AgentTools(df=df_citations)\n",
    "\n",
    "tools = [agent_tools.get_most_cited_countries, agent_tools.get_publications_per_year]\n",
    "\n",
    "# Tell the LLM which tools it can call\n",
    "llm_with_tools = model.bind_tools(tools)\n",
    "def chatbot(state: State):\n",
    "    print(state)\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "    \n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e7ea3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='\\nUse the right tool to answer the question asked enclosed in <query>.\\nIf further information is needed use the following tools [<bound method AgentTools.get_most_cited_countries of <__main__.AgentTools object at 0x7f09284857f0>>, <bound method AgentTools.get_publications_per_year of <__main__.AgentTools object at 0x7f09284857f0>>]\\n<query>\\nTell me if the country most cited in papers about stateless cloud computing and\\n</query>\\n/no_think\\n', additional_kwargs={}, response_metadata={}, id='4b4d83bb-409a-4de8-8bb3-4af985ef49cd')]}\n",
      "Uso de tool 1\n",
      "{'messages': [HumanMessage(content='\\nUse the right tool to answer the question asked enclosed in <query>.\\nIf further information is needed use the following tools [<bound method AgentTools.get_most_cited_countries of <__main__.AgentTools object at 0x7f09284857f0>>, <bound method AgentTools.get_publications_per_year of <__main__.AgentTools object at 0x7f09284857f0>>]\\n<query>\\nTell me if the country most cited in papers about stateless cloud computing and\\n</query>\\n/no_think\\n', additional_kwargs={}, response_metadata={}, id='4b4d83bb-409a-4de8-8bb3-4af985ef49cd'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-05-28T20:16:31.198474139Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2837498072, 'load_duration': 1747171512, 'prompt_eval_count': 313, 'prompt_eval_duration': 276610778, 'eval_count': 30, 'eval_duration': 813129571, 'model_name': 'qwen3'}, id='run-6c92cfe2-ea56-40a0-8ac2-9fe0ab8986c1-0', tool_calls=[{'name': 'get_most_cited_countries', 'args': {'text': 'stateless cloud computing'}, 'id': '25a924bc-6c66-4ddd-9202-1ccedce60ea8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 313, 'output_tokens': 30, 'total_tokens': 343}), ToolMessage(content='Most cited country is USA', name='get_most_cited_countries', id='dff9e773-e8ea-42d8-aaa5-32617ed6cd58', tool_call_id='25a924bc-6c66-4ddd-9202-1ccedce60ea8')]}\n"
     ]
    }
   ],
   "source": [
    "user_input = f\"\"\"\n",
    "Use the right tool to answer the question asked enclosed in <query>.\n",
    "If further information is needed use the following tools {tools}\n",
    "<query>\n",
    "Tell me if the country most cited in papers about stateless cloud computing and\n",
    "</query>\n",
    "/no_think\n",
    "\"\"\"\n",
    "response = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_input}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "286403f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The country most cited in papers about stateless cloud computing is the USA.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "91b6fdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='\\nUse the data enclosed in <context> to answer the question asked enclosed in <query>.\\nIf further information is needed use the following tools [<function get_most_cited_countries at 0x7f1b58f73920>, <function get_publications_per_year at 0x7f1b58f71080>]\\n<context>\\n \\n</context>\\n<query>\\nTell me the country most cited in papers about stateless cloud computing and te publications per year\\n</query>\\n/no_think\\n', additional_kwargs={}, response_metadata={}, id='dbb6c0b8-e81a-441a-bd97-f81bc26bf253')]}\n",
      "Assistant: \n",
      "tool 1\n",
      "tool 2\n",
      "Assistant: Don't have enough data\n",
      "{'messages': [HumanMessage(content='\\nUse the data enclosed in <context> to answer the question asked enclosed in <query>.\\nIf further information is needed use the following tools [<function get_most_cited_countries at 0x7f1b58f73920>, <function get_publications_per_year at 0x7f1b58f71080>]\\n<context>\\n \\n</context>\\n<query>\\nTell me the country most cited in papers about stateless cloud computing and te publications per year\\n</query>\\n/no_think\\n', additional_kwargs={}, response_metadata={}, id='dbb6c0b8-e81a-441a-bd97-f81bc26bf253'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-05-27T20:56:39.457287265Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3655785080, 'load_duration': 1780044351, 'prompt_eval_count': 304, 'prompt_eval_duration': 287162228, 'eval_count': 56, 'eval_duration': 1587990036, 'model_name': 'qwen3'}, id='run-294cdd5d-4c4b-48ff-b3db-7ef305894984-0', tool_calls=[{'name': 'get_most_cited_countries', 'args': {'text': 'stateless cloud computing'}, 'id': 'f6d69d60-1b7b-4675-a0e4-857c1ef6b77c', 'type': 'tool_call'}, {'name': 'get_publications_per_year', 'args': {'text': 'stateless cloud computing'}, 'id': 'e6292d37-9d2e-439a-86d8-61ebb99294bf', 'type': 'tool_call'}], usage_metadata={'input_tokens': 304, 'output_tokens': 56, 'total_tokens': 360}), ToolMessage(content='Here are the most cited countries in markdown format:  \\n| cited_country | citation_count |\\n|---------------|----------------|\\n| US            | 1845           |\\n| DE            | 203            |\\n| GB            | 199            |\\n| CN            | 198            |\\n| FR            | 134            |\\n| CH            | 119            |\\n| ES            | 116            |\\n| CA            | 97             |\\n| NL            | 88             |\\n| IT            | 87             |', name='get_most_cited_countries', id='91fb56c6-6519-41b1-8227-96d58cdd6a29', tool_call_id='f6d69d60-1b7b-4675-a0e4-857c1ef6b77c'), ToolMessage(content=\"Don't have enough data\", name='get_publications_per_year', id='0d4b30b2-3260-4999-9680-594f70f7db1e', tool_call_id='e6292d37-9d2e-439a-86d8-61ebb99294bf')]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99105/950956891.py:7: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  .agg(pl.count().alias(\"citation_count\"))\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The country most cited in papers about stateless cloud computing is the United States (US) with a citation count of 1845.\n",
      "\n",
      "However, I couldn't retrieve the publication count per year for \"stateless cloud computing\" due to insufficient data. Let me know if you'd like to explore other related topics!\n"
     ]
    }
   ],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "            \n",
    "stream_graph_updates(f\"\"\"\n",
    "Use the data enclosed in <context> to answer the question asked enclosed in <query>.\n",
    "If further information is needed use the following tools {tools}\n",
    "<context>\n",
    "{\"\"} \n",
    "</context>\n",
    "<query>\n",
    "Tell me the country most cited in papers about stateless cloud computing and te publications per year\n",
    "</query>\n",
    "/no_think\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "14631dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceedd674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
