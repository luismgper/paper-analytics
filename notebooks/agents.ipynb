{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc422e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.papers.io import db\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "# TODO variables de entorno y no usar caminos relativos\n",
    "EXTENDED_CRAWLER_DATA_PATH = os.getenv(\"EXTENDED_CRAWLER_DATA_PATH\")\n",
    "MILVUS_DB = os.getenv(\"MILVUS_DB\")\n",
    "MILVUS_COLLECTION = os.getenv(\"MILVUS_COLLECTION\")\n",
    "MILVUS_ALIAS = os.getenv(\"MILVUS_ALIAS\")\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\")\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\")\n",
    "\n",
    "milvus_client = db.Milvus(\n",
    "    # db=MILVUS_DB, \n",
    "    collection=MILVUS_COLLECTION, \n",
    "    alias=MILVUS_ALIAS,\n",
    "    host=MILVUS_HOST,\n",
    "    port=MILVUS_PORT, \n",
    "    new_collection=False\n",
    ")   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecb10c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Spot instances\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd723116",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_papers = milvus_client.search(\n",
    "    text=query,\n",
    "    output_fields=[\n",
    "        \"Title\",\n",
    "        \"TLDR\",\n",
    "        \"Abstract\",\n",
    "        \"KeyConcepts\",\n",
    "        \"Year\",\n",
    "        \"Conference\",\n",
    "        \"Summary\",\n",
    "        \"AuthorsAndInstitutions\"\n",
    "    ],\n",
    "    limit=100,\n",
    "    hybrid=True,\n",
    "    hybrid_fields=[\n",
    "        \"AbstractVector\", \n",
    "        \"TitleVector\", \n",
    "        \"TLDRVector\",\n",
    "        \"KeyConceptsVector\"\n",
    "    ],\n",
    "    expr=\"Year in ['2023']\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [paper[\"entity\"] for paper in nn_papers]\n",
    "for paper in papers:\n",
    "    print(paper[\"Year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e990bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87020ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = model.invoke([HumanMessage(content=\"hi!\")])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd34a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = []\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "response = model_with_tools.invoke([HumanMessage(content=\"Hi!\")])\n",
    "\n",
    "print(f\"ContentString: {response.content}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")\n",
    "\n",
    "response = model_with_tools.invoke([HumanMessage(content=\"What's the weather in SF?\")])\n",
    "\n",
    "print(f\"ContentString: {response.content}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba9a4a5",
   "metadata": {},
   "source": [
    "### Agente bÃ¡sico\n",
    "Las funciones a utilizar son proporcionada por medio de tools. Estas toman un input a partir de la query y son devueltos\n",
    "\n",
    "Se debe tipifizar la salida con pydantic, que es proporcionada al agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911aec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Base agent\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from pydantic import BaseModel\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"qwen3\")\n",
    "\n",
    "def get_person(person: str) -> str:  \n",
    "    \"\"\"Get person information.\"\"\"\n",
    "    return f\"Jaimito is my cousin, lives in Madrid, he is 7 years old\"\n",
    "\n",
    "class WhoIsResponse(BaseModel):\n",
    "    person: str\n",
    "    lives_in: str\n",
    "    age: int\n",
    "    \n",
    "tools = [get_person]\n",
    "agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    response_format=WhoIsResponse,\n",
    "    prompt=\"You are a helpful assistant that uses only the feedback you are provided\",  \n",
    ")\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Who is Jaimito?\"}]}\n",
    ")\n",
    "\n",
    "response[\"structured_response\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1390eea",
   "metadata": {},
   "source": [
    "### Minimal workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b9cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1:8b\")\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    print(\"weather\")\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "def who_is(text: str) -> str:  \n",
    "    \"\"\"who is person information.\"\"\"\n",
    "    print(\"person\")\n",
    "    return \"Jaimito is my cousin, lives in Madrid, he is 7 years old\"\n",
    "\n",
    "# class DescriptionResponse(BaseModel):\n",
    "#     person: str\n",
    "#     lives_in: str\n",
    "#     age: int\n",
    "\n",
    "# agent = create_react_agent(\n",
    "#     model=model,\n",
    "#     tools=[get_weather],\n",
    "#     prompt=\"You are a helpful assistant\"\n",
    "# )\n",
    "\n",
    "tools = []\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# Tell the LLM which tools it can call\n",
    "llm_with_tools = model.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    print(state)\n",
    "    return {\"messages\": [model.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "# Add tool node\n",
    "# tool_node = ToolNode(tools)\n",
    "# graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Add condition to call a node or another\n",
    "# graph_builder.add_conditional_edges(\n",
    "#     \"chatbot\",\n",
    "#     tools_condition,\n",
    "# )\n",
    "\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever\n",
    "# the node is used.\n",
    "# graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d9d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114266d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13daa938",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph_updates(\"Spot instances papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108cadc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d7791ba",
   "metadata": {},
   "source": [
    "### Prototipo de agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Base agent\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "query = \"Which papers were published in 2022 in nsdi related to serverless cloud computing? Select only those from Germany. Omit also citations from 2023\"\n",
    "model = ChatOllama(model=\"qwen3\", temperature=0, top_k=20, top_p=0.8)\n",
    "\n",
    "class FilterOptions(BaseModel):\n",
    "    \"\"\"\n",
    "    Class for filter option. \n",
    "    - value corresponds for to the value to be used in filtering\n",
    "    - equal indicates if the condition indicates equality\n",
    "    - citation indicates if this condition is referred only to cited papers\n",
    "    \"\"\"\n",
    "    value: str\n",
    "    equal: bool\n",
    "    citation: bool\n",
    "\n",
    "class FilterParameters(BaseModel):\n",
    "    main_concept: str = None\n",
    "    authors: list[FilterOptions] = None\n",
    "    institutions: list[FilterOptions] = None\n",
    "    countries: list[FilterOptions] = None\n",
    "    years: list[FilterOptions] = None\n",
    "    conferences: list[FilterOptions] = None\n",
    "    \n",
    "def get_query_filter_conditions(query: str, model: ChatOllama) -> FilterParameters:\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(\"\"\"\n",
    "        You are a helpful assistant that uses only the input the user provides. Your role is to help research paper data related to the topics the user asks about.\n",
    "        Fields with list of FilterOptions type must be filled as follows:\n",
    "        - value: The value of the field to be filled\n",
    "        - equal: False if it is asked to be different from the value\n",
    "        - citation: If the field value refers to paper citations\n",
    "        Return JSON with (if not mentioned, leave it as None):\n",
    "        - main_concept (string). Topic of research\n",
    "        - years (list[FilterOptions]). Publish years\n",
    "        - authors (list[FilterOptions]). List of paper authors\n",
    "        - institutions (list[FilterOptions]). All must be valid institutions laike universities or corporations.\n",
    "        - countries (list[FilterOptions]). All must be valid countries. Translate them to usual abbreviaions like US for United Staes of America or DE for Germany\n",
    "        - conferences (list[FilterOptions]). List of valid conferences: IEEECloud, Middleware, SIGCOMM, eurosys\n",
    "        Example response for 2024 papers about spot instances with 100+ citations published in middleware. Authors must be from Harvard University. Cited papers must not be from Canada:\n",
    "        {{\n",
    "            \"main_concept\": \"spot instances\",\n",
    "            \"authors\": None,\n",
    "            \"institutions\": [{{ \"value\": \"Harvard University\", \"equal\": True, \"citation\": False}}],\n",
    "            \"countries\": [{{\"value\": \"CA\", \"equal\": False, \"citation\": True}}],\n",
    "            \"years\": [{{\"value\": \"2025\", \"equal\": True, \"citation\": False}}],\n",
    "            \"conferences\": [{{\"value\": \"Middleware\", \"equal\": True, \"citation\": False}}],\n",
    "        }}\n",
    "        /no_think\n",
    "        \"\"\"),\n",
    "        HumanMessage(query),\n",
    "    ]\n",
    "    # response = model.with_structured_output(FilterParameters).invoke(messages)\n",
    "    response = model.invoke(messages)\n",
    "    print(response)\n",
    "    # return response\n",
    "    messages_for_structure = [\n",
    "        messages[0],\n",
    "        HumanMessage(f\"\"\" \n",
    "        Return JSON with (if not mentioned, leave it as None):\n",
    "        - value: The value of the field to be filled\n",
    "        - equal: False if it is asked to be different from the value\n",
    "        - citation: If the field value refers to paper citations\n",
    "        Return JSON with (if not mentioned, leave it as None):\n",
    "        - main_concept (string). Topic of research\n",
    "        - years (list[FilterOptions]). Publish years\n",
    "        - authors (list[FilterOptions]). List of paper authors\n",
    "        - institutions (list[FilterOptions]). All must be valid institutions laike universities or corporations.\n",
    "        - countries (list[FilterOptions]). All must be valid countries. Translate them to usual abbreviaions like US for United Staes of America or DE for Germany\n",
    "        - conferences (list[FilterOptions]). List of valid conferences: IEEECloud, Middleware, SIGCOMM, eurosys\n",
    "        Using these previous result:\n",
    "        {response}\n",
    "        /no_think\n",
    "        \"\"\"\n",
    "        )\n",
    "    ]\n",
    "    structured_response = model.with_structured_output(FilterParameters).invoke(messages_for_structure)\n",
    "    print(structured_response)\n",
    "    \n",
    "    return structured_response\n",
    "\n",
    "filter_conditions = get_query_filter_conditions(query, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678d4f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_expr_for_filter_field(filter_options: list[FilterOptions], field_name: str, json_field: bool = False) -> str:\n",
    "    expr = \"\"\n",
    "    in_expr = []\n",
    "    not_in_expr = []\n",
    "\n",
    "    if not filter_options:\n",
    "        return \"\"\n",
    "    \n",
    "    for filter_option in filter_options:\n",
    "        if not filter_option.citation:\n",
    "            if filter_option.equal:\n",
    "                in_expr.append(filter_option.value)\n",
    "            else:\n",
    "                not_in_expr.append(filter_option.value)\n",
    "            \n",
    "    if len(in_expr) > 0:\n",
    "        if len(expr) == 0:\n",
    "            if json_field:\n",
    "                expr = f\"json_contains_any({field_name}, {in_expr})\"\n",
    "            else:\n",
    "                expr = f\"{field_name} in {in_expr}\"\n",
    "        else:\n",
    "            if len(expr) == 0:\n",
    "                if json_field:            \n",
    "                    expr += f\"and json_contains_any({field_name}, {in_expr})\"\n",
    "                else:\n",
    "                    expr += f\"and {field_name} in {in_expr}\"\n",
    "\n",
    "    if len(not_in_expr) > 0:\n",
    "        if len(expr) == 0:\n",
    "            if json_field:\n",
    "                expr = f\"not json_contains_any({field_name}, {not_in_expr})\"\n",
    "            else:\n",
    "                expr = f\"not {field_name} in {not_in_expr}\"\n",
    "        else:\n",
    "            if len(expr) == 0:\n",
    "                if json_field:            \n",
    "                    expr += f\"and not json_contains_any({field_name}, {not_in_expr})\"\n",
    "                else:\n",
    "                    expr += f\"and not {field_name} in {not_in_expr}\"\n",
    "    return expr\n",
    "\n",
    "expr_years = build_expr_for_filter_field(filter_options=filter_conditions.years, field_name=\"Year\")\n",
    "expr_authors = build_expr_for_filter_field(filter_options=filter_conditions.authors, field_name=\"Authors\", json_field=True)\n",
    "expr_countries = build_expr_for_filter_field(filter_options=filter_conditions.countries, field_name=\"Countries\", json_field=True)\n",
    "expr_institution = build_expr_for_filter_field(filter_options=filter_conditions.institutions, field_name=\"Institutions\", json_field=True)\n",
    "expr_conferences = build_expr_for_filter_field(filter_options=filter_conditions.conferences, field_name=\"Conferences\")\n",
    "\n",
    "expr = \"\"\n",
    "if len(expr_years) > 0:\n",
    "    if len(expr) == 0:\n",
    "        expr = expr_years\n",
    "    else:\n",
    "        expr += \" and \" + expr_years\n",
    "        \n",
    "if len(expr_authors) > 0:\n",
    "    if len(expr) == 0:\n",
    "        expr = expr_authors\n",
    "    else:\n",
    "        expr += \" and \" + expr_authors\n",
    "        \n",
    "if len(expr_countries) > 0:\n",
    "    if len(expr) == 0:\n",
    "        expr = expr_countries\n",
    "    else:\n",
    "        expr += \" and \" + expr_countries\n",
    "        \n",
    "if len(expr_institution) > 0:\n",
    "    if len(expr) == 0:\n",
    "        expr = expr_institution\n",
    "    else:\n",
    "        expr += \" and \" + expr_institution\n",
    "        \n",
    "# if len(expr_conferences) > 0:\n",
    "#     if len(expr) == 0:\n",
    "#         expr = expr_conferences\n",
    "#     else:\n",
    "#         expr += \" and \" + expr_conferences                                \n",
    "\n",
    "print(expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429cff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_papers = milvus_client.search(\n",
    "    text=filter_conditions.main_concept,\n",
    "    output_fields=[\n",
    "        \"Title\",\n",
    "        \"TLDR\",\n",
    "        \"Abstract\",\n",
    "        \"KeyConcepts\",\n",
    "        \"Year\",\n",
    "        \"Conference\",\n",
    "        \"Summary\",\n",
    "        \"AuthorsAndInstitutions\"\n",
    "    ],\n",
    "    limit=100,\n",
    "    hybrid=True,\n",
    "    hybrid_fields=[\n",
    "        \"AbstractVector\", \n",
    "        \"TitleVector\", \n",
    "        \"TLDRVector\",\n",
    "        \"KeyConceptsVector\"\n",
    "    ],\n",
    "    # expr=expr\n",
    "    expr=\"\"\n",
    ")\n",
    "if len(nn_papers) > 0:\n",
    "    print(len(nn_papers))\n",
    "    print(nn_papers[0][\"entity\"][\"AuthorsAndInstitutions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7283bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicCheck(BaseModel):\n",
    "    is_valid: bool\n",
    "    # reason: str\n",
    "    \n",
    "\n",
    "def check_paper_topic(topic: str, paper: dict, model: ChatOllama) -> dict:\n",
    "    model_with_structure = model.with_structured_output(TopicCheck)\n",
    "    title = paper[\"Title\"]\n",
    "    abstract = paper[\"Abstract\"] if paper[\"Abstract\"] else \"\"\n",
    "    tldr = paper[\"TLDR\"] if paper[\"TLDR\"] else \"\"\n",
    "    messages = [\n",
    "        SystemMessage(\"\"\"You are a strict assistant. Your role is to help determine if papers discuss the topic the user asks about. \n",
    "        Only accept those that strictly mention the topic since a mistake will kill all your family. If in doubt is it better to determine a paper as not valid.\n",
    "        Return JSON with (if not mentioned, leave it as None):\n",
    "        - is_valid (bool). True if the paper data is about the topic requested\n",
    "        Example response for paper with abstract 'Kappa proposes a framework for simplified serverless development using checkpointing to handle timeouts and providing concurrency mechanisms for parallel' and topic 'serverless development':\n",
    "        {{\n",
    "            \"is_valid\": true,\n",
    "        }}  \n",
    "        \"\"\"),\n",
    "        HumanMessage(f\"\"\"Determine if the following paper content is about the topic: <topic>{topic}</topic>\\n\n",
    "        Title: {title}\\n\n",
    "        TLDR: {tldr}\\n\n",
    "        Abstract: {abstract}\\n\n",
    "        \"\"\"),\n",
    "    ]\n",
    "    response = model_with_structure.invoke(messages)\n",
    "    return response\n",
    "\n",
    "def filter_papers(topic: str, papers: list[dict], model: ChatOllama) -> list[str]:\n",
    "    valid_papers = []\n",
    "    for paper in papers:\n",
    "        entity = paper[\"entity\"]\n",
    "        \n",
    "        check_result = check_paper_topic(topic, entity, model)\n",
    "        print(check_result)\n",
    "        if check_result.is_valid:\n",
    "            valid_papers.append(entity)\n",
    "            \n",
    "    return valid_papers\n",
    "            \n",
    "valid_papers = filter_papers(topic=filter_conditions.main_concept, papers=nn_papers, model=model)\n",
    "print(len(valid_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e1ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"qwen3:0.6b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paralallell\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "valid_papers = []\n",
    "index = 0\n",
    "map_chain_elements = {}\n",
    "for paper in nn_papers:\n",
    "    entity = paper[\"entity\"]\n",
    "    \n",
    "    topic = filter_conditions.main_concept\n",
    "    \n",
    "    model_with_structure = model.with_structured_output(TopicCheck)\n",
    "    title = entity[\"Title\"]\n",
    "    abstract = entity[\"Abstract\"] if entity[\"Abstract\"] else \"\"\n",
    "    tldr = entity[\"TLDR\"] if entity[\"TLDR\"] else \"\"\n",
    "    messages = [\n",
    "        SystemMessage(\"\"\"You are a strict assistant. Your role is to help determine if papers discuss the topic the user asks about. \n",
    "        Only accept those that strictly mention the topic since a mistake will kill all your family. If in doubt is it better to determine a paper as not valid.\n",
    "        Return JSON with (if not mentioned, leave it as None):\n",
    "        - is_valid (bool). True if the paper data is about the topic requested\n",
    "        Example response for paper with abstract 'Kappa proposes a framework for simplified serverless development using checkpointing to handle timeouts and providing concurrency mechanisms for parallel' and topic 'serverless development':\n",
    "        {{\n",
    "            \"is_valid\": true,\n",
    "        }}  \n",
    "        \"\"\"),\n",
    "        HumanMessage(f\"\"\"Determine if the following paper content is about the topic: <topic>{topic}</topic>\\n\n",
    "        Title: {title}\\n\n",
    "        TLDR: {tldr}\\n\n",
    "        Abstract: {abstract}\\n\n",
    "        \"\"\")\n",
    "    ]\n",
    "    key = f\"chain_{index}\"\n",
    "    index += 1\n",
    "    chain = (\n",
    "        ChatPromptTemplate.from_messages(messages)\n",
    "        | model_with_structure\n",
    "    )\n",
    "    # print(chain)\n",
    "    map_chain_elements[key] = chain\n",
    "    # print(map_chain_elements)\n",
    "\n",
    "# print(map_chain_elements)\n",
    "map_chain = RunnableParallel(map_chain_elements)\n",
    "response = map_chain.invoke({})\n",
    "print(response)\n",
    "valid_papers = []\n",
    "for index, paper in enumerate(nn_papers):\n",
    "    chain_i = f\"chain_{index}\"\n",
    "    if response[chain_i].is_valid:\n",
    "        valid_papers.append(paper)\n",
    "        \n",
    "print(len(valid_papers))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7082aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = nn_papers[0][\"entity\"]\n",
    "\n",
    "print(entity[\"Title\"])\n",
    "print(entity[\"Abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"qwen3\", temperature=0)\n",
    "\n",
    "class AggregationInQueryChecker(BaseModel):\n",
    "    requests_aggregation: bool\n",
    "    aggregations_requested: list[str]\n",
    "    \n",
    "\n",
    "def check_aggregations_requested(query: str, model: ChatOllama) -> dict:\n",
    "    model_with_structure = model.with_structured_output(AggregationInQueryChecker)\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(\"\"\"You are a helpful AI assistant that uses only the feedback you are provided. Your role is to help in custom searches of scientific papers. Determine if in the provided query data aggregations are requested.\n",
    "        An example would be \"Provide the most cited papers of every country relating spot instances\", which is requesting an aggregation by country\n",
    "        Return JSON with the following properties:\n",
    "        - requests_aggregation (bool). True if the query requests data aggregations\n",
    "        - aggregations_requested (list[string]). List containing the expected aggregations. The possible values are the following ones: citations, papers, country, conference, institution, author\n",
    "        Example response for the query 'Provide the most cited papers of every country relating spot instances in conference Middleware':\n",
    "        {{\n",
    "            \"requests_aggregation\": true,\n",
    "            \"aggregations_requested\": [\"citations\", \"country\"]\n",
    "        }}  \n",
    "        \"\"\"),\n",
    "        HumanMessage(f\"\"\"Determine the aggregations requested in the following query: <query>{query}</query>:\\n\"\"\"),\n",
    "    ]\n",
    "    response = model_with_structure.invoke(messages)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc21045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "check_aggregations_requested(query=query, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade9719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.papers.domain.citations_analyzer import CitationAnalyzer\n",
    "import polars as pl\n",
    "\n",
    "### Dynamic query\n",
    "neo4j_client = db.Neo4j(\"bolt://localhost:7687\", \"neo4j\", \"password\", \"middleware\")\n",
    "citations_analyzer = CitationAnalyzer(graph_client=neo4j_client)\n",
    "\n",
    "papers = [paper[\"entity\"] for paper in nn_papers]\n",
    "df_citations = citations_analyzer.process_papers(papers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_citations.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_conditions = get_query_filter_conditions(query, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e2de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def dynamic_filter(filters: list[dict], df: pl.DataFrame) -> pl.DataFrame:\n",
    "    df_filtered = df\n",
    "    for filter in filters:\n",
    "        print(filter)\n",
    "        if filter[\"equal\"]:\n",
    "            df_filtered = df_filtered.filter(\n",
    "                pl.col(filter[\"field\"]).is_in(filter[\"values\"])\n",
    "            )\n",
    "        else:\n",
    "            df_filtered = df_filtered.filter(\n",
    "                ~pl.col(filter[\"field\"]).is_in(filter[\"values\"])\n",
    "            )\n",
    "    return df_filtered\n",
    "\n",
    "def dynamic_aggregation(columns: list[str], df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return (\n",
    "        df.group_by(columns)\n",
    "        .agg(pl.count().alias(\"total_citations\"))\n",
    "    )\n",
    "    \n",
    "    # authors: list[FilterOptions] = None\n",
    "    # institutions: list[FilterOptions] = None\n",
    "    # countries: list[FilterOptions] = None\n",
    "    # years: list[FilterOptions] = None\n",
    "    # conferences: list[FilterOptions] = None\n",
    "def process_paper_data(filter_parameters: FilterParameters, df:pl.DataFrame) -> pl.DataFrame:\n",
    "    df_filters = []\n",
    "    if filter_parameters.authors and len(filter_parameters.authors) > 0:\n",
    "        df_filter_authors = map_filter_options(filter_options=filter_parameters.authors, source_field=\"source_author\", cited_field=\"cited_author\")\n",
    "        for filter in df_filter_authors:\n",
    "            df_filters.append(filter)\n",
    "            \n",
    "    if filter_parameters.institutions and len(filter_parameters.institutions) > 0:\n",
    "        df_filter_institutions = map_filter_options(filter_options=filter_parameters.institutions, source_field=\"source_institution\", cited_field=\"cited_institution\")\n",
    "        for filter in df_filter_institutions:\n",
    "            df_filters.append(filter)\n",
    "            model = ChatOllama(model=\"qwen3\", temperature=0)\n",
    "\n",
    "class AggregationInQueryChecker(BaseModel):\n",
    "    requests_aggregation: bool\n",
    "    aggregations_requested: list[str]\n",
    "    \n",
    "\n",
    "def check_aggregations_requested(query: str, model: ChatOllama) -> dict:\n",
    "    model_with_structure = model.with_structured_output(AggregationInQueryChecker)\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(\"\"\"You are a helpful AI assistant that uses only the feedback you are provided. Your role is to help in custom searches of scientific papers. Determine if in the provided query data aggregations are requested.\n",
    "        An example would be \"Provide the most cited papers of every country relating spot instances\", which is requesting an aggregation by country\n",
    "        Return JSON with the following properties:\n",
    "        - requests_aggregation (bool). True if the query requests data aggregations\n",
    "        - aggregations_requested (list[string]). List containing the expected aggregations. The possible values are the following ones: citations, papers, country, conference, institution, author\n",
    "        Example response for the query 'Provide the most cited papers of every country relating spot instances in conference Middleware':\n",
    "        {{\n",
    "            \"requests_aggregation\": true,\n",
    "            \"aggregations_requested\": [\"citations\", \"country\"]\n",
    "        }}  \n",
    "        \"\"\"),\n",
    "        HumanMessage(f\"\"\"Determine the aggregations requested in the following query: <query>{query}</query>:\\n\"\"\"),\n",
    "    ]\n",
    "    response = model_with_structure.invoke(messages)\n",
    "    return response\n",
    "    if filter_parameters.countries and len(filter_parameters.countries) > 0:\n",
    "        df_filter_countries = map_filter_options(filter_options=filter_parameters.countries, source_field=\"source_country\", cited_field=\"cited_country\")\n",
    "        for filter in df_filter_countries:\n",
    "            df_filters.append(filter)\n",
    "            \n",
    "    if filter_parameters.years and len(filter_parameters.years) > 0:\n",
    "        df_filter_years = map_filter_options(filter_options=filter_parameters.years, source_field=\"source_year\", cited_field=\"cited_year\")\n",
    "        for filter in df_filter_years:\n",
    "            df_filters.append(filter)                                    \n",
    "            \n",
    "    if filter_parameters.conferences and len(filter_parameters.conferences) > 0:\n",
    "        df_filter_conferences = map_filter_options(filter_options=filter_parameters.conferences, source_field=\"source_conference\", cited_field=\"cited_conference\")\n",
    "        for filter in df_filter_conferences:\n",
    "            df_filters.append(filter)      \n",
    "            \n",
    "    df_filtered = dynamic_filter(filters=df_filters, df=df)\n",
    "    df_aggregated = dynamic_aggregation(columns=[\"cited_country\"], df=df_filtered)\n",
    "    return df_aggregated           \n",
    "\n",
    "def map_filter_options(filter_options: list[FilterOptions], source_field, cited_field: str) -> list[dict]:\n",
    "    mapping = {\n",
    "        \"cited_equal\": [],\n",
    "        \"cited_not_equal\": [],\n",
    "        \"source_equal\": [],\n",
    "        \"source_not_equal\": [],\n",
    "    }\n",
    "    df_filters = []\n",
    "    for filter_option in filter_options:\n",
    "        if filter_option.citation:\n",
    "            mapping[\"cited_equal\"].append(filter_option.value) if filter_option.equal else mapping[\"cited_not_equal\"].append(filter_option.value)\n",
    "        else:            \n",
    "            mapping[\"source_equal\"].append(filter_option.value) if filter_option.equal else mapping[\"source_not_equal\"].append(filter_option.value)\n",
    "            \n",
    "    if len(mapping[\"source_equal\"]) > 0:\n",
    "        df_filters.append({\"field\": source_field, \"values\": mapping[\"source_equal\"], \"equal\": True})\n",
    "        \n",
    "    if len(mapping[\"source_not_equal\"]) > 0:\n",
    "        df_filters.append({\"field\": source_field, \"values\": mapping[\"source_not_equal\"], \"equal\": False})\n",
    "        \n",
    "    if len(mapping[\"cited_equal\"]) > 0:            \n",
    "        df_filters.append({\"field\": cited_field, \"values\": mapping[\"cited_equal\"], \"equal\": True})\n",
    "        \n",
    "    if len(mapping[\"cited_not_equal\"]) > 0:\n",
    "        df_filters.append({\"field\": cited_field, \"values\": mapping[\"cited_not_equal\"], \"equal\": False})    \n",
    "    return df_filters\n",
    "    \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b2ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_paper_data(filter_parameters=filter_conditions, df=df_citations).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa817eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregated = dynamic_aggregation(\n",
    "    columns=[\"source_year\", \"source_title\", \"source_conference\"],\n",
    "    df=df_filtered\n",
    ")\n",
    "df_aggregated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6a8fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302680f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.agents import create_structured_chat_agent\n",
    "\n",
    "\n",
    "model = ChatOllama(model=\"qwen3\", temperature=0)\n",
    "\n",
    "agent = create_structured_chat_agent(\n",
    "    llm = model,\n",
    "    tools=[],\n",
    "    prompt\n",
    ")\n",
    "\n",
    "def check_aggregations_requested(query: str, model: ChatOllama) -> dict:\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad7273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_structured_chat_agent\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field\n",
    "import polars as pl\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Define your tools with structured inputs\n",
    "class CitationAnalysisTool(BaseModel):\n",
    "    pass  # No input needed for this example\n",
    "\n",
    "class PublicationTrendTool(BaseModel):\n",
    "    year_range: list[int] = Field(\n",
    "        None, \n",
    "        description=\"Optional year range filter [start_year, end_year]\"\n",
    "    )\n",
    "\n",
    "def get_most_cited_countries(df: pl.DataFrame) -> dict:\n",
    "    \"\"\"Get top cited countries from the dataframe\"\"\"\n",
    "    return (\n",
    "        df.filter(pl.col(\"cited_country\") != \"UNKNOWN\")\n",
    "        .group_by(\"cited_country\")\n",
    "        .agg(pl.len().alias(\"citation_count\"))\n",
    "        .sort(\"citation_count\", descending=True)\n",
    "        .to_dicts()\n",
    "    )\n",
    "\n",
    "def get_publications_per_year(df: pl.DataFrame, year_range: list = None) -> dict:\n",
    "    \"\"\"Get publication count by year with optional filtering\"\"\"\n",
    "    base = df.filter(pl.col(\"source_year\").cast(int).is_not_null())\n",
    "    \n",
    "    if year_range:\n",
    "        base = base.filter(\n",
    "            pl.col(\"source_year\").cast(int).is_between(year_range[0], year_range[1])\n",
    "        )\n",
    "    \n",
    "    return (\n",
    "        base.group_by(\"source_year\")\n",
    "        .agg(pl.len().alias(\"publication_count\"))\n",
    "        .sort(\"source_year\")\n",
    "        .to_dicts()\n",
    "    )\n",
    "\n",
    "# Create structured tools\n",
    "tools = [\n",
    "    StructuredTool.from_function(\n",
    "        func=lambda _: get_most_cited_countries(df_citations),\n",
    "        name=\"CountryCitationAnalysis\",\n",
    "        description=\"Analyze citation patterns by country\",\n",
    "        args_schema=CitationAnalysisTool\n",
    "    ),\n",
    "    StructuredTool.from_function(\n",
    "        func=lambda yr: get_publications_per_year(df_citations, yr),\n",
    "        name=\"PublicationTrendAnalysis\",\n",
    "        description=\"Analyze publication trends over years\",\n",
    "        args_schema=PublicationTrendTool\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize the agent\n",
    "llm = ChatOllama(model=\"qwen3\", temperature=0, top_k=20, top_p=0.8)\n",
    "system = '''Respond to the human as helpfully and accurately as possible. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n",
    "\n",
    "Valid \"action\" values: \"Final Answer\" or {tool_names}\n",
    "\n",
    "Provide only ONE action per $JSON_BLOB, as shown:\n",
    "\n",
    "```\n",
    "{{model = ChatOllama(model=\"qwen3\", temperature=0)\n",
    "```\n",
    "\n",
    "Follow this format:\n",
    "\n",
    "Question: input question to answer\n",
    "Thought: consider previous and subsequent steps\n",
    "Action:\n",
    "```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: action result\n",
    "... (repeat Thought/Action/Observation N times)\n",
    "Thought: I know what to respond\n",
    "Action:\n",
    "```\n",
    "{{\n",
    "  \"action\": \"Final Answer\",\n",
    "  \"action_input\": \"Final response to human\"\n",
    "}}\n",
    "\n",
    "Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation /no_think'''\n",
    "\n",
    "human = '''{input}\n",
    "\n",
    "{agent_scratchpad}\n",
    "\n",
    "(reminder to respond in a JSON blob no matter what) /no_think'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        # MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", human),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent = create_structured_chat_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "def process_query(query: str):\n",
    "    response = agent.invoke({\n",
    "        \"input\": f\"Analyze this research query: {query}\",\n",
    "        \"intermediate_steps\": []\n",
    "    })\n",
    "    return response\n",
    "\n",
    "# Test queries\n",
    "print(process_query(\"Which countries are most cited in cloud research?\"))\n",
    "print(process_query(\"Show me publication trends between 2010 and 2020\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_cited_countries(text) -> str:\n",
    "    \"\"\"Get top cited countries from the dataframe\"\"\"\n",
    "    print(\"tool 1\")\n",
    "    df = (\n",
    "        df_citations.filter(pl.col(\"cited_country\") != \"UNKNOWN\")\n",
    "        .group_by([\"cited_country\"])\\\n",
    "        .agg(pl.count().alias(\"citation_count\"))\\\n",
    "        .sort(\"citation_count\", descending=True)\\\n",
    "        .limit(10)\n",
    "    )\n",
    "    with pl.Config(\n",
    "        tbl_formatting=\"MARKDOWN\",\n",
    "        tbl_hide_column_data_types=True,\n",
    "        tbl_hide_dataframe_shape=True,\n",
    "    ):    \n",
    "        return \"Here are the most cited countries in markdown format:  \\n\" + str((df).head(10))\n",
    "def get_publications_per_year(text) -> dict:\n",
    "    \"\"\"Get publication count by year with optional filtering\"\"\"\n",
    "    print(\"tool 2\")\n",
    "    return \"Don't have enough data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800396e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model = ChatOllama(model=\"llama3.2\", temperature=0.6, top_k=20, top_p=0.8)\n",
    "tools = [get_most_cited_countries, get_publications_per_year]\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "data = df_citations.to_dicts()\n",
    "\n",
    "response = model_with_tools.invoke([HumanMessage(f\"\"\"\n",
    "Use the data enclosed in <context> to answer the question asked enclosed in <query>.\n",
    "If further information is needed use the following tools {tools}\n",
    "<context>\n",
    "{\"\"}\n",
    "</context>\n",
    "<query>\n",
    "Tell me the country most publications per year\n",
    "</query>\n",
    "/no_think\n",
    "\"\"\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3344c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_citations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375eea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_cited_countries(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0987241",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_citations.filter(pl.col(\"cited_country\") != \"UNKNOWN\")\\\n",
    "    .group_by([\"cited_title\", \"cited_country\"])\\\n",
    "    .agg(pl.count().alias(\"citation_count\"))\\\n",
    "    .group_by([\"cited_country\"])\\\n",
    "    .agg(pl.sum(\"citation_count\").alias(\"citation_count\"))\\\n",
    "    .sort(\"citation_count\", descending=True)\\\n",
    "    .limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ed01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_citations.filter(pl.col(\"cited_country\") != \"UNKNOWN\")\\\n",
    "    .unique([\"source_title\",\"cited_title\", \"cited_country\"])\\\n",
    "    .group_by([\"source_title\",\"cited_title\", \"cited_country\"])\\\n",
    "    .agg(pl.count().alias(\"citation_count\"))\\\n",
    "    .sort(\"source_title\", descending=True)\n",
    "\n",
    "with pl.Config(\n",
    "    tbl_formatting=\"MARKDOWN\",\n",
    "    tbl_hide_column_data_types=True,\n",
    "    tbl_hide_dataframe_shape=True,\n",
    "):\n",
    "    st = str((df).head(10))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c9356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_ollama import ChatOllama\n",
    "import polars as pl\n",
    "\n",
    "model = ChatOllama(model=\"qwen3\", temperature=0.2)\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "class AgentTools:\n",
    "    def __init__(self, df: pl.DataFrame):\n",
    "        self.df = df\n",
    "    def get_most_cited_countries(self, text: str):\n",
    "        \"\"\"Get top cited countries from the dataframe\"\"\"\n",
    "        print(\"Uso de tool 1\")\n",
    "        return \"Most cited country is USA\"\n",
    "    def get_publications_per_year(self, text: str):\n",
    "        \"\"\"Get publication count by year with optional filtering\"\"\"\n",
    "        print(\"Uso de tool 2\")\n",
    "        return \"Most publications done in 2022\"\n",
    "    \n",
    "    \n",
    "agent_tools = AgentTools(df=df_citations)\n",
    "\n",
    "tools = [agent_tools.get_most_cited_countries, agent_tools.get_publications_per_year]\n",
    "\n",
    "# Tell the LLM which tools it can call\n",
    "llm_with_tools = model.bind_tools(tools)\n",
    "def chatbot(state: State):\n",
    "    print(state)\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "    \n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7ea3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = f\"\"\"\n",
    "Use the right tool to answer the question asked enclosed in <query>.\n",
    "If further information is needed use the following tools {tools}\n",
    "<query>\n",
    "Tell me if the country most cited in papers about stateless cloud computing and\n",
    "</query>\n",
    "/no_think\n",
    "\"\"\"\n",
    "response = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_input}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286403f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b6fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "            \n",
    "stream_graph_updates(f\"\"\"\n",
    "Use the data enclosed in <context> to answer the question asked enclosed in <query>.\n",
    "If further information is needed use the following tools {tools}\n",
    "<context>\n",
    "{\"\"} \n",
    "</context>\n",
    "<query>\n",
    "Tell me the country most cited in papers about stateless cloud computing and te publications per year\n",
    "</query>\n",
    "/no_think\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14631dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceedd674",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "import json\n",
    "\n",
    "llm = ChatOllama(model=\"qwen3\", temperature=0.2, top_k=20, top_p=0.8)\n",
    "\n",
    "def extract_filters(query):\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Extract structured parameters from this query. Return ONLY JSON.\n",
    "        Available filters:\n",
    "          - year (list of years)\n",
    "          - country (use 2-letter abbreviations: US, DE, CA, etc.)\n",
    "          - institution (list of universities, companies or corporations)\n",
    "          - conference (list of conferences where the paper was published)\n",
    "          - author (list of authors who published papers)\n",
    "          - Aggregation: 'most cited' implies country aggregation\n",
    "\n",
    "        Operators: $eq, $ne, $in, $nin, $gt, $gte, $lt, $lte, $between\n",
    "\n",
    "        Output structure:\n",
    "        {{\n",
    "          \"topic\": \"research topic of query\"\n",
    "          \"source_filters\": {{\n",
    "            \"field1\": {{\"$operator\": value}},\n",
    "            \"field2\": {{\"$operator\": value}}\n",
    "          }},\n",
    "          \"cited_filters\": {{\n",
    "            \"field3\": {{\"$operator\": value}}\n",
    "          }},\n",
    "          \"aggregation\": {{\n",
    "            \"type\": \"count\", \n",
    "            \"group_by\": \"field\",\n",
    "            \"sort\": \"desc\"\n",
    "          }}\n",
    "        }}\n",
    "        \n",
    "        Special handling:\n",
    "        - if $between operator first and last value are the same, use $eq instead\n",
    "\n",
    "        Important: Convert all country names to 2-letter abbreviations! If filters apply to citaitons add them to cited_filters else to source_filters\n",
    "        Example conversion:\n",
    "          \"USA\" â \"US\"\n",
    "          \"Germany\" â \"DE\"\n",
    "          \"Canada\" â \"CA\"\n",
    "\n",
    "        Query: {query}\n",
    "        Output: \n",
    "        /no_think\"\"\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"query\": query})\n",
    "    try:\n",
    "        filters = json.loads(response.content.partition('</think>\\n\\n')[2])\n",
    "        # Normalize country abbreviations in filters\n",
    "        # for filter_type in [\"source_filters\", \"cited_filters\"]:\n",
    "        #     if filter_type in filters and \"country\" in filters[filter_type]:\n",
    "        #         country_spec = filters[filter_type][\"country\"]\n",
    "                # if \"$ne\" in country_spec:\n",
    "                #     country_spec[\"$ne\"] = normalize_country(country_spec[\"$ne\"])\n",
    "                # if \"$in\" in country_spec:\n",
    "                #     country_spec[\"$in\"] = [normalize_country(c) for c in country_spec[\"$in\"]]\n",
    "        return filters\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"source_filters\": {}, \"cited_filters\": {}, \"aggregation\": None}    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19452192",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_filters(\"MAin papers about stateful serverless computing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd6afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORTED_FIELDS = {\n",
    "    \"year\": {\"type\": \"int\", \"milvus_field\": \"year\", \"neo4j_field\": \"year\"},\n",
    "    \"country\": {\"type\": \"str\", \"milvus_field\": \"country\", \"neo4j_field\": \"country\"},\n",
    "    \"authors\": {\"type\": \"list\", \"milvus_field\": \"authors\", \"neo4j_field\": \"authors\"},\n",
    "    \"venue\": {\"type\": \"str\", \"milvus_field\": \"venue\", \"neo4j_field\": \"venue\"},\n",
    "    \"keywords\": {\"type\": \"list\", \"milvus_field\": \"keywords\", \"neo4j_field\": \"keywords\"},\n",
    "    \"citation_count\": {\"type\": \"int\", \"milvus_field\": \"citation_count\", \"neo4j_field\": \"citationCount\"},\n",
    "    \"document_type\": {\"type\": \"str\", \"milvus_field\": \"doc_type\", \"neo4j_field\": \"documentType\"}\n",
    "}\n",
    "\n",
    "COUNTRY_MAPPING = {\n",
    "    \"USA\": \"US\", \"UNITED STATES\": \"US\", \"AMERICA\": \"US\",\n",
    "    \"DEUTSCHLAND\": \"DE\", \"GERMANY\": \"DE\",\n",
    "    \"CANADA\": \"CA\", \"FRANCE\": \"FR\", \"UK\": \"GB\", \n",
    "    \"UNITED KINGDOM\": \"GB\", \"ENGLAND\": \"GB\", \"CHINA\": \"CN\",\n",
    "    \"JAPAN\": \"JP\", \"AUSTRALIA\": \"AU\", \"INDIA\": \"IN\", \"BRAZIL\": \"BR\"\n",
    "}\n",
    "\n",
    "OPERATOR_KEYS_TO_SYMBOLS = {\n",
    "    \"$gt\": \">\", \n",
    "    \"$gte\": \">=\", \n",
    "    \"$lt\": \"<\", \n",
    "    \"$lte\": \"<=\",\n",
    "}\n",
    "\n",
    "def normalize_field_value(field, value):\n",
    "    \"\"\"Normalize values based on field type and special handling\"\"\"\n",
    "    # if field == \"country\":\n",
    "    #     value = value.upper().strip() if isinstance(value, str) else [item.upper().strip() for item in value]\n",
    "    #     return COUNTRY_MAPPING.get(value, value[:2].upper())\n",
    "    \n",
    "    if SUPPORTED_FIELDS[field][\"type\"] == \"list\" and not isinstance(value, list):\n",
    "        return [v.strip() for v in value.split(\",\")]\n",
    "    \n",
    "    return value\n",
    "\n",
    "def validate_and_normalize_filters(filters):\n",
    "    \"\"\"Validate and normalize filter values\"\"\"\n",
    "    normalized = {\"source_filters\": {}, \"cited_filters\": {}}\n",
    "    \n",
    "    for filter_type in [\"source_filters\", \"cited_filters\"]:\n",
    "        if filter_type not in filters:\n",
    "            continue\n",
    "            \n",
    "        for field, conditions in filters[filter_type].items():\n",
    "            if field not in SUPPORTED_FIELDS:\n",
    "                continue\n",
    "            \n",
    "            normalized_conds = {}\n",
    "            for op, value in conditions.items():\n",
    "                # Handle special operators\n",
    "                if op == \"$between\" and isinstance(value, list) and len(value) == 2:\n",
    "                    normalized_conds[\"$gte\"] = value[0]\n",
    "                    normalized_conds[\"$lte\"] = value[1]\n",
    "                else:\n",
    "                    # Normalize single values\n",
    "                    if not isinstance(value, list) or op in [\"$in\", \"$nin\"]:\n",
    "                        value = normalize_field_value(field, value)\n",
    "                    # Normalize list values\n",
    "                    elif isinstance(value, list):\n",
    "                        value = [normalize_field_value(field, v) for v in value]\n",
    "                    \n",
    "                    normalized_conds[op] = value\n",
    "            \n",
    "            if normalized_conds:\n",
    "                normalized[filter_type][field] = normalized_conds\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d4008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_milvus_expr(filters):\n",
    "    \"\"\"Build Milvus boolean expression from filters\"\"\"\n",
    "    conditions = []\n",
    "    \n",
    "    for field, spec in filters.items():\n",
    "        if field not in SUPPORTED_FIELDS:\n",
    "            continue\n",
    "            \n",
    "        milvus_field = SUPPORTED_FIELDS[field][\"milvus_field\"]\n",
    "        field_type = SUPPORTED_FIELDS[field][\"type\"]\n",
    "        \n",
    "        for op, value in spec.items():\n",
    "            print(f\"op: {op}, value: {value}\")\n",
    "            # Handle different operators\n",
    "            if op == \"$eq\":\n",
    "                if field_type == \"str\":\n",
    "                    conditions.append(f\"{milvus_field} == '{value}'\")\n",
    "                else:\n",
    "                    conditions.append(f\"{milvus_field} == {value}\")\n",
    "                    \n",
    "            elif op == \"$ne\":\n",
    "                if field_type == \"str\":\n",
    "                    conditions.append(f\"{milvus_field} != '{value}'\")\n",
    "                else:\n",
    "                    conditions.append(f\"{milvus_field} != {value}\")\n",
    "                    \n",
    "            elif op == \"$in\":\n",
    "                if field_type == \"str\":\n",
    "                    items = [f\"'{v}'\" for v in value]\n",
    "                else:\n",
    "                    items = [str(v) for v in value]\n",
    "                conditions.append(f\"{milvus_field} in [{','.join(items)}]\")\n",
    "                \n",
    "            elif op == \"$nin\":\n",
    "                if field_type == \"str\":\n",
    "                    items = [f\"'{v}'\" for v in value]\n",
    "                else:\n",
    "                    items = [str(v) for v in value]\n",
    "                conditions.append(f\"not {milvus_field} in [{','.join(items)}]\")\n",
    "                \n",
    "            elif op in [\"$gt\", \"$gte\", \"$lt\", \"$lte\"]:\n",
    "                operator_symbol = OPERATOR_KEYS_TO_SYMBOLS[op]\n",
    "                conditions.append(f\"{milvus_field} {operator_symbol} {value}\")\n",
    "    \n",
    "    return \" and \".join(conditions) if conditions else None\n",
    "\n",
    "def build_neo4j_conditions(filters, alias=\"cited\"):\n",
    "    \"\"\"Build WHERE conditions for Neo4j\"\"\"\n",
    "    conditions = []\n",
    "    params = {}\n",
    "    param_count = 0\n",
    "    \n",
    "    for field, spec in filters.items():\n",
    "        \n",
    "        print(f\"field  {field}, spec {spec}\")\n",
    "        if field not in SUPPORTED_FIELDS:\n",
    "            continue\n",
    "            \n",
    "        neo4j_field = SUPPORTED_FIELDS[field][\"neo4j_field\"]\n",
    "        field_type = SUPPORTED_FIELDS[field][\"type\"]\n",
    "        \n",
    "        for op, value in spec.items():\n",
    "            param_name = f\"param{param_count}\"\n",
    "            param_count += 1\n",
    "            \n",
    "            print(f\"op {op}, value {value}\")\n",
    "            \n",
    "            if op == \"$eq\":\n",
    "                if field_type == \"str\":\n",
    "                    conditions.append(f\"{alias}.{neo4j_field} = ${param_name}\")\n",
    "                else:\n",
    "                    conditions.append(f\"{alias}.{neo4j_field} = ${param_name}\")\n",
    "                params[param_name] = value\n",
    "                \n",
    "            elif op == \"$ne\":\n",
    "                if field_type == \"str\":\n",
    "                    conditions.append(f\"{alias}.{neo4j_field} <> ${param_name}\")\n",
    "                else:\n",
    "                    conditions.append(f\"{alias}.{neo4j_field} <> ${param_name}\")\n",
    "                params[param_name] = value\n",
    "                \n",
    "            elif op == \"$in\":\n",
    "                conditions.append(f\"{alias}.{neo4j_field} IN ${param_name}\")\n",
    "                params[param_name] = value\n",
    "                \n",
    "            elif op == \"$nin\":\n",
    "                conditions.append(f\"NOT {alias}.{neo4j_field} IN ${param_name}\")\n",
    "                params[param_name] = value\n",
    "                \n",
    "            elif op == \"$gt\":\n",
    "                conditions.append(f\"{alias}.{neo4j_field} > ${param_name}\")\n",
    "                params[param_name] = value\n",
    "                \n",
    "            elif op == \"$gte\":\n",
    "                conditions.append(f\"{alias}.{neo4j_field} >= ${param_name}\")\n",
    "                params[param_name] = value\n",
    "                \n",
    "            elif op == \"$lt\":\n",
    "                conditions.append(f\"{alias}.{neo4j_field} < ${param_name}\")\n",
    "                params[param_name] = value\n",
    "                \n",
    "            elif op == \"$lte\":\n",
    "                conditions.append(f\"{alias}.{neo4j_field} <= ${param_name}\")\n",
    "                params[param_name] = value\n",
    "    \n",
    "    where_clause = \" AND \".join(conditions)\n",
    "    return f\"WHERE {where_clause}\" if conditions else \"\", params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76873076",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"stateful serverless computing most cited countries\"\n",
    "filters = extract_filters(query)\n",
    "print(filters)\n",
    "\n",
    "valid_filters = validate_and_normalize_filters(filters)\n",
    "print(valid_filters)\n",
    "\n",
    "expr = build_milvus_expr(valid_filters.get(\"source_filters\", {}))\n",
    "print(expr)\n",
    "\n",
    "neo4j_where = build_neo4j_conditions(valid_filters.get(\"cited_filters\", {}))\n",
    "print(neo4j_where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ca49b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caddc2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_milvus_expr(filters):\n",
    "    conditions = []\n",
    "    for field, spec in filters.items():\n",
    "        if field == \"year\" and \"$in\" in spec:\n",
    "            years = spec[\"$in\"]\n",
    "            conditions.append(f\"year in {years}\")\n",
    "        elif field == \"country\":\n",
    "            if \"$ne\" in spec:\n",
    "                abbr = normalize_country(spec[\"$ne\"])\n",
    "                conditions.append(f\"country != '{abbr}'\")\n",
    "            if \"$in\" in spec:\n",
    "                abbrs = [normalize_country(c) for c in spec[\"$in\"]]\n",
    "                conditions.append(f\"country in {abbrs}\")\n",
    "    return \" and \".join(conditions) if conditions else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0932b3bd",
   "metadata": {},
   "source": [
    "# Agent with multiple conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f46f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Provide me a list of the main papers about serverless cloud computing\"\n",
    "\n",
    "\n",
    "from src.papers.io import db\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import polars as pl\n",
    "\n",
    "load_dotenv()\n",
    "# TODO variables de entorno y no usar caminos relativos\n",
    "EXTENDED_CRAWLER_DATA_PATH = os.getenv(\"EXTENDED_CRAWLER_DATA_PATH\")\n",
    "MILVUS_DB = os.getenv(\"MILVUS_DB\")\n",
    "MILVUS_COLLECTION = os.getenv(\"MILVUS_COLLECTION\")\n",
    "MILVUS_ALIAS = os.getenv(\"MILVUS_ALIAS\")\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\")\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\")\n",
    "\n",
    "milvus_client = db.Milvus(\n",
    "    # db=MILVUS_DB, \n",
    "    collection=MILVUS_COLLECTION, \n",
    "    alias=MILVUS_ALIAS,\n",
    "    host=MILVUS_HOST,\n",
    "    port=MILVUS_PORT, \n",
    "    new_collection=False\n",
    ")   \n",
    "\n",
    "nn_papers = milvus_client.search(\n",
    "    text=query,\n",
    "    output_fields=[\n",
    "        \"Title\",\n",
    "        \"TLDR\",\n",
    "        \"Abstract\",\n",
    "        \"KeyConcepts\",\n",
    "        \"Year\",\n",
    "        \"Conference\",\n",
    "        \"Summary\",\n",
    "        \"AuthorsAndInstitutions\"\n",
    "    ],\n",
    "    limit=100,\n",
    "    hybrid=True,\n",
    "    hybrid_fields=[\n",
    "        \"AbstractVector\", \n",
    "        \"TitleVector\", \n",
    "        \"TLDRVector\",\n",
    "        \"KeyConceptsVector\"\n",
    "    ],\n",
    "    # expr=expr\n",
    "    expr=\"\"\n",
    ")\n",
    "\n",
    "df = pl.DataFrame(nn_papers)\n",
    "\n",
    "##################################################################\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_ollama import ChatOllama\n",
    "import polars as pl\n",
    "from src.papers.domain.agent_tools import AgentTools\n",
    "\n",
    "agent_tools = AgentTools(df)\n",
    "\n",
    "model = ChatOllama(model=\"qwen3\", temperature=0.2)\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "class AgentTools:\n",
    "    def __init__(self, df: pl.DataFrame):\n",
    "        self.df = df\n",
    "    def get_most_cited_countries(self, text: str):\n",
    "        \"\"\"Get top cited countries from the dataframe\"\"\"\n",
    "        print(\"Uso de tool 1\")\n",
    "        return \"Most cited country is USA\"\n",
    "    def get_publications_per_year(self, text: str):\n",
    "        \"\"\"Get publication count by year with optional filtering\"\"\"\n",
    "        print(\"Uso de tool 2\")\n",
    "        return \"Most publications done in 2022\"\n",
    "    \n",
    "    \n",
    "tools = [agent_tools.get_most_cited_countries, agent_tools.get_publications_per_year, agent_tools.get_other_citation_requested]\n",
    "\n",
    "# Tell the LLM which tools it can call\n",
    "llm_with_tools = model.bind_tools(tools)\n",
    "def chatbot(state: State):\n",
    "    print(state)\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "    \n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a6671b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [SystemMessage(content='\\nYou are an intelligent AI assisstant which is responsible of helping the user consult research paper data.\\nYour role is to use the provided tools to retrieve the needed information about papers and answer using it.\\nIMPORTANT: Never use the same tool twice for the same question. \\n', additional_kwargs={}, response_metadata={}, id='7052b6e1-d2fa-476a-af6b-777c47ca6f88'), HumanMessage(content='\\nUse the right tool to answer the question asked enclosed in <query>. If not adecuate tool is found, use get_other_citation_requested\\nIf further information is needed use the following tools [<bound method AgentTools.get_most_cited_countries of <src.papers.domain.agent_tools.AgentTools object at 0x713320393610>>, <bound method AgentTools.get_publications_per_year of <src.papers.domain.agent_tools.AgentTools object at 0x713320393610>>, <bound method AgentTools.get_other_citation_requested of <src.papers.domain.agent_tools.AgentTools object at 0x713320393610>>]\\n<query>\\nProvide me a list of the main papers about serverless cloud computing\\n</query>\\n/no_think\\n', additional_kwargs={}, response_metadata={}, id='4cbe0cec-6025-450b-b3cf-7ae4dd9d43ab')]}\n",
      "other tool user\n",
      "{'messages': [SystemMessage(content='\\nYou are an intelligent AI assisstant which is responsible of helping the user consult research paper data.\\nYour role is to use the provided tools to retrieve the needed information about papers and answer using it.\\nIMPORTANT: Never use the same tool twice for the same question. \\n', additional_kwargs={}, response_metadata={}, id='7052b6e1-d2fa-476a-af6b-777c47ca6f88'), HumanMessage(content='\\nUse the right tool to answer the question asked enclosed in <query>. If not adecuate tool is found, use get_other_citation_requested\\nIf further information is needed use the following tools [<bound method AgentTools.get_most_cited_countries of <src.papers.domain.agent_tools.AgentTools object at 0x713320393610>>, <bound method AgentTools.get_publications_per_year of <src.papers.domain.agent_tools.AgentTools object at 0x713320393610>>, <bound method AgentTools.get_other_citation_requested of <src.papers.domain.agent_tools.AgentTools object at 0x713320393610>>]\\n<query>\\nProvide me a list of the main papers about serverless cloud computing\\n</query>\\n/no_think\\n', additional_kwargs={}, response_metadata={}, id='4cbe0cec-6025-450b-b3cf-7ae4dd9d43ab'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-06-01T17:33:38.023251984Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2332979561, 'load_duration': 13609680, 'prompt_eval_count': 489, 'prompt_eval_duration': 276694202, 'eval_count': 33, 'eval_duration': 2035310143, 'model_name': 'qwen3'}, id='run-d051ef09-f258-4922-90b6-40a16215d5df-0', tool_calls=[{'name': 'get_other_citation_requested', 'args': {'text': 'main papers about serverless cloud computing'}, 'id': 'd6a5e2de-b8c5-4701-a2e1-72bc453239ee', 'type': 'tool_call'}], usage_metadata={'input_tokens': 489, 'output_tokens': 33, 'total_tokens': 522}), ToolMessage(content='No data found for question \\'main papers about serverless cloud computing\\' \\n only the following data is available:  \\n| id                 | distance | entity                          |\\n|--------------------|----------|---------------------------------|\\n| 458270836533358234 | 0.058856 | {\"IEEEcloud\",\"This paper evaluâ¦ |\\n| 458270836533359116 | 0.054302 | {\"ic2e\",\"This paper investigatâ¦ |\\n| 458270836533353152 | 0.05407  | {\"icdcs\",\"Researchers identifyâ¦ |\\n| 458270836533358902 | 0.053616 | {\"ic2e\",\"Serverless computing â¦ |\\n| 458270836533354788 | 0.05319  | {\"ccgrid\",\"This paper proposesâ¦ |\\n| â¦                  | â¦        | â¦                               |\\n| 458270836533352530 | 0.014744 | {\"icdcs\",\"Gillis is a serverleâ¦ |\\n| 458270836533358588 | 0.014445 | {\"IEEEcloud\",\"A Hybrid Serverlâ¦ |\\n| 458270836533357378 | 0.01423  | {\"IEEEcloud\",\"This paper discuâ¦ |\\n| 458270836533359512 | 0.013616 | {\"cloud\",\"Distributed computinâ¦ |\\n| 458270836533358264 | 0.013412 | {\"IEEEcloud\",\"This paper propoâ¦ |', name='get_other_citation_requested', id='b6802642-64ad-45a2-bba4-e7256f9f42db', tool_call_id='d6a5e2de-b8c5-4701-a2e1-72bc453239ee')]}\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_MESSAGE=\"\"\"\n",
    "You are an intelligent AI assisstant which is responsible of helping the user consult research paper data.\n",
    "Your role is to use the provided tools to retrieve the needed information about papers and answer using it.\n",
    "IMPORTANT: Never use the same tool twice for the same question. \n",
    "\"\"\"\n",
    "\n",
    "USER_MESSAGE = f\"\"\"\n",
    "Use the right tool to answer the question asked enclosed in <query>. If not adecuate tool is found, use get_other_citation_requested\n",
    "If further information is needed use the following tools {tools}\n",
    "<query>\n",
    "{query}\n",
    "</query>\n",
    "/no_think\n",
    "\"\"\"\n",
    "\n",
    "response = graph.invoke({\n",
    "    \"messages\": [\n",
    "        \n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE},\n",
    "    ]\n",
    "        })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eefd967c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content.partition(\u001b[33m'\u001b[39m\u001b[33m</think>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)[\u001b[32m2\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].content.partition('</think>\\n\\n')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e0c1d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (100, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>distance</th></tr><tr><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>458270836533358234</td><td>0.058856</td></tr><tr><td>458270836533359116</td><td>0.054302</td></tr><tr><td>458270836533353152</td><td>0.05407</td></tr><tr><td>458270836533358902</td><td>0.053616</td></tr><tr><td>458270836533354788</td><td>0.05319</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>458270836533352530</td><td>0.014744</td></tr><tr><td>458270836533358588</td><td>0.014445</td></tr><tr><td>458270836533357378</td><td>0.01423</td></tr><tr><td>458270836533359512</td><td>0.013616</td></tr><tr><td>458270836533358264</td><td>0.013412</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (100, 2)\n",
       "ââââââââââââââââââââââ¬âââââââââââ\n",
       "â id                 â distance â\n",
       "â ---                â ---      â\n",
       "â i64                â f64      â\n",
       "ââââââââââââââââââââââªâââââââââââ¡\n",
       "â 458270836533358234 â 0.058856 â\n",
       "â 458270836533359116 â 0.054302 â\n",
       "â 458270836533353152 â 0.05407  â\n",
       "â 458270836533358902 â 0.053616 â\n",
       "â 458270836533354788 â 0.05319  â\n",
       "â â¦                  â â¦        â\n",
       "â 458270836533352530 â 0.014744 â\n",
       "â 458270836533358588 â 0.014445 â\n",
       "â 458270836533357378 â 0.01423  â\n",
       "â 458270836533359512 â 0.013616 â\n",
       "â 458270836533358264 â 0.013412 â\n",
       "ââââââââââââââââââââââ´âââââââââââ"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"id\", \"distance\").head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd465526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (99, 2)\n",
      "ââââââââââââââââââââââ¬âââââââââââ\n",
      "â id                 â distance â\n",
      "â ---                â ---      â\n",
      "â i64                â f64      â\n",
      "ââââââââââââââââââââââªâââââââââââ¡\n",
      "â 458270836533358234 â 0.058856 â\n",
      "â 458270836533359116 â 0.054302 â\n",
      "â 458270836533353152 â 0.05407  â\n",
      "â 458270836533358902 â 0.053616 â\n",
      "â 458270836533354788 â 0.05319  â\n",
      "â 458270836533359866 â 0.052466 â\n",
      "â 458270836533358086 â 0.052189 â\n",
      "â 458270836533358272 â 0.050725 â\n",
      "â 458270836533357728 â 0.048036 â\n",
      "â 458270836533358592 â 0.046806 â\n",
      "â 458270836533354766 â 0.046581 â\n",
      "â 458270836533354786 â 0.046484 â\n",
      "â 458270836533357952 â 0.046318 â\n",
      "â 458270836533359836 â 0.045724 â\n",
      "â 458270836533360152 â 0.045277 â\n",
      "â 458270836533359780 â 0.043835 â\n",
      "â 458270836533358084 â 0.041841 â\n",
      "â 458270836533361664 â 0.040527 â\n",
      "â 458270836533356580 â 0.039294 â\n",
      "â 458270836533360006 â 0.039155 â\n",
      "â 458270836533357836 â 0.038822 â\n",
      "â 458270836533361522 â 0.038731 â\n",
      "â 458270836533361658 â 0.037164 â\n",
      "â 458270836533359906 â 0.03709  â\n",
      "â 458270836533358990 â 0.037087 â\n",
      "â 458270836533355058 â 0.036877 â\n",
      "â 458270836533353064 â 0.036816 â\n",
      "â 458270836533352308 â 0.036628 â\n",
      "â 458270836533354828 â 0.036147 â\n",
      "â 458270836533361636 â 0.034825 â\n",
      "â 458270836533359904 â 0.034495 â\n",
      "â 458270836533357918 â 0.034188 â\n",
      "â 458270836533358372 â 0.03354  â\n",
      "â 458270836533358348 â 0.033182 â\n",
      "â 458270836533358988 â 0.032386 â\n",
      "â 458270836533354980 â 0.03227  â\n",
      "â 458270836533354914 â 0.032072 â\n",
      "â 458270836533358374 â 0.031877 â\n",
      "â 458270836533354844 â 0.031857 â\n",
      "â 458270836533352982 â 0.03053  â\n",
      "â 458270836533357988 â 0.03005  â\n",
      "â 458270836533354418 â 0.028387 â\n",
      "â 458270836533354834 â 0.027758 â\n",
      "â 458270836533354494 â 0.027363 â\n",
      "â 458270836533355138 â 0.027264 â\n",
      "â 458270836533358888 â 0.027212 â\n",
      "â 458270836533355108 â 0.027027 â\n",
      "â 458270836533362570 â 0.026805 â\n",
      "â 458270836533361454 â 0.026781 â\n",
      "â 458270836533358052 â 0.02641  â\n",
      "â 458270836533360114 â 0.025975 â\n",
      "â 458270836533359072 â 0.025912 â\n",
      "â 458270836533358166 â 0.025734 â\n",
      "â 458270836533354790 â 0.025645 â\n",
      "â 458270836533354712 â 0.025194 â\n",
      "â 458270836533360002 â 0.025035 â\n",
      "â 458270836533359134 â 0.024924 â\n",
      "â 458270836533353420 â 0.024846 â\n",
      "â 458270836533354838 â 0.024724 â\n",
      "â 458270836533360060 â 0.024503 â\n",
      "â 458270836533350210 â 0.024104 â\n",
      "â 458270836533359944 â 0.023889 â\n",
      "â 458270836533358482 â 0.023791 â\n",
      "â 458270836533353372 â 0.02329  â\n",
      "â 458270836533361050 â 0.022631 â\n",
      "â 458270836533352776 â 0.021898 â\n",
      "â 458270836533354792 â 0.021869 â\n",
      "â 458270836533360148 â 0.021777 â\n",
      "â 458270836533357892 â 0.02104  â\n",
      "â 458270836533353546 â 0.020995 â\n",
      "â 458270836533350096 â 0.020928 â\n",
      "â 458270836533354988 â 0.020822 â\n",
      "â 458270836533354992 â 0.020822 â\n",
      "â 458270836533357320 â 0.019645 â\n",
      "â 458270836533359140 â 0.019583 â\n",
      "â 458270836533358566 â 0.019373 â\n",
      "â 458270836533354548 â 0.01885  â\n",
      "â 458270836533353514 â 0.018591 â\n",
      "â 458270836533359262 â 0.018398 â\n",
      "â 458270836533354656 â 0.018207 â\n",
      "â 458270836533353512 â 0.018046 â\n",
      "â 458270836533362626 â 0.017853 â\n",
      "â 458270836533359044 â 0.017568 â\n",
      "â 458270836533354364 â 0.017355 â\n",
      "â 458270836533352320 â 0.017059 â\n",
      "â 458270836533359064 â 0.017044 â\n",
      "â 458270836533359778 â 0.016811 â\n",
      "â 458270836533359868 â 0.016484 â\n",
      "â 458270836533354836 â 0.016339 â\n",
      "â 458270836533353460 â 0.016331 â\n",
      "â 458270836533354990 â 0.016291 â\n",
      "â 458270836533361112 â 0.016287 â\n",
      "â 458270836533357914 â 0.015937 â\n",
      "â 458270836533356386 â 0.015716 â\n",
      "â 458270836533360052 â 0.014885 â\n",
      "â 458270836533352530 â 0.014744 â\n",
      "â 458270836533358588 â 0.014445 â\n",
      "â 458270836533357378 â 0.01423  â\n",
      "â 458270836533359512 â 0.013616 â\n",
      "ââââââââââââââââââââââ´âââââââââââ\n"
     ]
    }
   ],
   "source": [
    "with pl.Config(tbl_rows=-1):\n",
    "    print(df.select(\"id\", \"distance\").head(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bbfa1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (99, 2)\n",
      "ââââââââââââââââââââââ¬âââââââââââ\n",
      "â id                 â distance â\n",
      "â ---                â ---      â\n",
      "â i64                â f64      â\n",
      "ââââââââââââââââââââââªâââââââââââ¡\n",
      "â 458270836533358234 â 0.058856 â\n",
      "â 458270836533359116 â 0.054302 â\n",
      "â 458270836533353152 â 0.05407  â\n",
      "â 458270836533358902 â 0.053616 â\n",
      "â 458270836533354788 â 0.05319  â\n",
      "â â¦                  â â¦        â\n",
      "â 458270836533360052 â 0.014885 â\n",
      "â 458270836533352530 â 0.014744 â\n",
      "â 458270836533358588 â 0.014445 â\n",
      "â 458270836533357378 â 0.01423  â\n",
      "â 458270836533359512 â 0.013616 â\n",
      "ââââââââââââââââââââââ´âââââââââââ\n"
     ]
    }
   ],
   "source": [
    "print(df.select(\"id\", \"distance\").head(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55995f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
